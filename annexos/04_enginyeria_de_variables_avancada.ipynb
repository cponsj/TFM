{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2f06da-e12d-40de-a884-c3b17efad992",
   "metadata": {},
   "source": [
    "## <b>3.3 PREPROCESSAMENT I ANÀLISI DE DADES</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cce20-7050-40cd-94c5-e0fdf77fa94e",
   "metadata": {},
   "source": [
    "### <b>3.3.4 Enginyeria de variables avançada</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2334a-8960-4097-bdb6-bc2710ff8b71",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.1 Objectiu i setup</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38da0f1a-dafd-4757-94fe-0dd7373cfbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregat: (105555, 44)\n",
      "\n",
      "Distribució train/test:\n",
      "set_type\n",
      "train    69740\n",
      "test     35815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "df_freq_base: sense columnes duplicades (24 columnes).\n",
      "\n",
      "df_sev_base: sense columnes duplicades (18 columnes).\n",
      "\n",
      "df_ratio_base: sense columnes duplicades (24 columnes).\n",
      "\n",
      "Nuls a df_freq_base:\n",
      "Policy_duration    70408\n",
      "dtype: int64\n",
      "\n",
      "Nuls a df_sev_base:\n",
      "Policy_duration    11936\n",
      "dtype: int64\n",
      "\n",
      "Nuls a df_ratio_base:\n",
      "Policy_duration    70408\n",
      "dtype: int64\n",
      "\n",
      "3.3.4.1 complet.\n",
      " - df_freq_base: predictors ex ante, sense leakage.\n",
      " - df_sev_base : severitat condicionada a sinistre, sense variables derivades del target.\n",
      " - df_ratio_base: dataset específic per al model de ràtio econòmica.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.1 ENGINYERIA DE VARIABLES AVANÇADA\n",
    "# SETUP INICIAL I DEFINICIÓ DE DATASETS PER MODEL\n",
    "# ============================================================\n",
    "# En aquest script inicio la fase d’enginyeria de variables avançada.\n",
    "# El meu objectiu aquí és preparar, de manera neta i traçable, els\n",
    "# datasets base que utilitzaré posteriorment en cada tipus de model.\n",
    "#\n",
    "# Concretament:\n",
    "#   - Carrego el dataset final preprocessat del pipeline anterior.\n",
    "#   - Defineixo clarament quines variables entren a cada model:\n",
    "#       * Freqüència: probabilitat de sinistre anual (Has_claims_year).\n",
    "#       * Severitat: cost del sinistre, només quan hi ha sinistre.\n",
    "#       * Ràtio econòmica: Claims_to_premium_ratio com a target independent.\n",
    "#   - Reconstrueixo l’any de pòlissa i el split temporal train/test.\n",
    "#   - Creo datasets base sense leakage des de l’origen.\n",
    "#   - Verifico duplicats, nuls i coherència abans d’exportar.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ajusto opcions de visualització per facilitar inspecció manual\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Funció auxiliar: control de columnes duplicades\n",
    "# ------------------------------------------------------------\n",
    "# Aquesta funció la faig servir com a mecanisme defensiu.\n",
    "# Si en algun punt s’han colat columnes duplicades (per merges,\n",
    "# concatenacions o errors previs), aquí les detecto i les elimino.\n",
    "def ensure_no_duplicate_columns(df_in: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    dup_mask = df_in.columns.duplicated()\n",
    "    if dup_mask.any():\n",
    "        dup_cols = df_in.columns[dup_mask]\n",
    "        print(f\"\\nATENCIÓ: columnes duplicades detectades a {name}: {list(dup_cols)}\")\n",
    "        df_out = df_in.loc[:, ~dup_mask].copy()\n",
    "        print(f\"   → Eliminades {df_in.shape[1] - df_out.shape[1]} columnes duplicades.\")\n",
    "        return df_out\n",
    "    else:\n",
    "        print(f\"\\n{name}: sense columnes duplicades ({df_in.shape[1]} columnes).\")\n",
    "        return df_in\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega del dataset canònic\n",
    "# ------------------------------------------------------------\n",
    "# Carrego el dataset final del pipeline (ja net, ordenat i validat).\n",
    "data_path = \"transformed_motor_insurance.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, sep=\",\", encoding=\"utf-8\")\n",
    "print(\"Dataset carregat:\", df.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Definició de variables clau\n",
    "# ------------------------------------------------------------\n",
    "# Defineixo explícitament noms de columnes importants\n",
    "# per evitar errors de string repetits més endavant.\n",
    "id_col = \"ID\"\n",
    "\n",
    "target_freq  = \"Has_claims_year\"\n",
    "target_sev   = \"Cost_claims_year\"\n",
    "target_ratio = \"Claims_to_premium_ratio\"\n",
    "\n",
    "# Predictors ex ante per a FREQÜÈNCIA\n",
    "# Aquí m’asseguro de no incloure cap variable ex post ni derivada del cost.\n",
    "freq_features_base = [\n",
    "    \"Driver_age\", \"Licence_age\", \"Vehicle_age\",\n",
    "    \"Has_lapse\", \"Policy_duration\",\n",
    "    \"Second_driver\", \"Area\", \"Type_risk\", \"Type_fuel\",\n",
    "    \"Has_claims_history\",\n",
    "    \"Value_vehicle\", \"Power\", \"Premium\",\n",
    "    \"Seniority\", \"Policies_in_force\",\n",
    "    \"Distribution_channel\"\n",
    "]\n",
    "\n",
    "# Predictors ex ante per a SEVERITAT (només quan hi ha sinistre)\n",
    "# Important: no incloc cap variable que utilitzi el cost del sinistre\n",
    "# ni cap ràtio econòmica derivada del target.\n",
    "sev_features_base = [\n",
    "    \"Vehicle_age\", \"Value_vehicle\", \"Power\",\n",
    "    \"Type_risk\", \"Area\", \"Policy_duration\",\n",
    "    \"Weight\", \"Cylinder_capacity\", \"Premium\", \"Length\"\n",
    "]\n",
    "\n",
    "# Variables de control i traçabilitat\n",
    "# Aquestes no són predictors de negoci, però m’interessa tenir-les\n",
    "# per auditoria, diagnòstic i control de qualitat.\n",
    "control_features = [\n",
    "    \"Licence_incoherent_flag\",\n",
    "    \"Policy_incoherent_flag\",\n",
    "    \"Lapse_incoherent_flag\",\n",
    "    \"Length_missing_flag\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Any de pòlissa i partició temporal\n",
    "# ------------------------------------------------------------\n",
    "# Reconstrueixo l’any de pòlissa a partir de Date_last_renewal\n",
    "# per assegurar coherència amb l’EDA i amb la validació temporal.\n",
    "date_col = \"Date_last_renewal\"\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "df[\"Policy_year\"] = df[date_col].dt.year\n",
    "df = df.dropna(subset=[\"Policy_year\"]).copy()\n",
    "df[\"Policy_year\"] = df[\"Policy_year\"].astype(int)\n",
    "\n",
    "# Defineixo el split temporal exactament igual que abans:\n",
    "#   train: anys < 2018\n",
    "#   test : any 2018\n",
    "df[\"set_type\"] = np.where(df[\"Policy_year\"] < 2018, \"train\", \"test\")\n",
    "\n",
    "print(\"\\nDistribució train/test:\")\n",
    "print(df[\"set_type\"].value_counts())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) DATASET BASE DE FREQÜÈNCIA (sense leakage)\n",
    "# ------------------------------------------------------------\n",
    "# Construeixo el dataset base per al model de freqüència.\n",
    "# Inclou:\n",
    "#   - identificador\n",
    "#   - info temporal mínima\n",
    "#   - predictors ex ante\n",
    "#   - target de freqüència\n",
    "#   - flags de control\n",
    "freq_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +\n",
    "    freq_features_base +\n",
    "    [target_freq] +\n",
    "    control_features\n",
    ")\n",
    "\n",
    "# Elimino possibles duplicats de la llista de columnes\n",
    "freq_vars = list(dict.fromkeys(freq_vars))\n",
    "# Em quedo només amb les columnes que realment existeixen al dataframe\n",
    "freq_vars = [v for v in freq_vars if v in df.columns]\n",
    "\n",
    "df_freq_base = df[freq_vars].copy()\n",
    "df_freq_base = ensure_no_duplicate_columns(df_freq_base, \"df_freq_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) DATASET BASE DE SEVERITAT (només sinistres, sense leakage)\n",
    "# ------------------------------------------------------------\n",
    "# Per a severitat, em quedo només amb registres amb:\n",
    "#   - Has_claims_year = 1\n",
    "#   - cost positiu i informat\n",
    "mask_sev = (\n",
    "    (df[target_freq] == 1) &\n",
    "    (df[target_sev].notna()) &\n",
    "    (df[target_sev] > 0)\n",
    ")\n",
    "\n",
    "df_sev = df.loc[mask_sev].copy()\n",
    "\n",
    "sev_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +\n",
    "    sev_features_base +\n",
    "    [target_sev] +\n",
    "    control_features\n",
    ")\n",
    "\n",
    "sev_vars = list(dict.fromkeys(sev_vars))\n",
    "sev_vars = [v for v in sev_vars if v in df_sev.columns]\n",
    "\n",
    "df_sev_base = df_sev[sev_vars].copy()\n",
    "df_sev_base = ensure_no_duplicate_columns(df_sev_base, \"df_sev_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) DATASET BASE DE RÀTIO ECONÒMICA (model independent)\n",
    "# ------------------------------------------------------------\n",
    "# Aquest dataset el preparo per separat, ja que la ràtio econòmica\n",
    "# és un target diferent i no condicionat a tenir sinistre.\n",
    "ratio_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +\n",
    "    freq_features_base +\n",
    "    [target_ratio] +\n",
    "    control_features\n",
    ")\n",
    "\n",
    "ratio_vars = list(dict.fromkeys(ratio_vars))\n",
    "ratio_vars = [v for v in ratio_vars if v in df.columns]\n",
    "\n",
    "df_ratio_base = df[ratio_vars].copy()\n",
    "df_ratio_base = ensure_no_duplicate_columns(df_ratio_base, \"df_ratio_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Comprovació ràpida de nuls\n",
    "# ------------------------------------------------------------\n",
    "# Faig una comprovació ràpida de nuls per detectar problemes evidents\n",
    "# abans de passar a la fase de modelització.\n",
    "def quick_missing_report(df_in, name):\n",
    "    missing = df_in.isna().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    print(f\"\\nNuls a {name}:\")\n",
    "    if missing.empty:\n",
    "        print(\"  Cap nul no estructural.\")\n",
    "    else:\n",
    "        print(missing)\n",
    "\n",
    "quick_missing_report(df_freq_base,  \"df_freq_base\")\n",
    "quick_missing_report(df_sev_base,   \"df_sev_base\")\n",
    "quick_missing_report(df_ratio_base, \"df_ratio_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Exportació datasets base\n",
    "# ------------------------------------------------------------\n",
    "# Exporto tots els datasets base a una carpeta específica\n",
    "# perquè quedin llestos per a la fase de modelització.\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "df_freq_base.to_csv(\"data/processed/df_freq_base.csv\", index=False)\n",
    "df_sev_base.to_csv(\"data/processed/df_sev_base.csv\", index=False)\n",
    "df_ratio_base.to_csv(\"data/processed/df_ratio_base.csv\", index=False)\n",
    "\n",
    "print(\"\\n3.3.4.1 complet.\")\n",
    "print(\" - df_freq_base: predictors ex ante, sense leakage.\")\n",
    "print(\" - df_sev_base : severitat condicionada a sinistre, sense variables derivades del target.\")\n",
    "print(\" - df_ratio_base: dataset específic per al model de ràtio econòmica.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ca2af-440b-46d2-9f8a-540d17b291f0",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.2 Transformacions de variables numèriques</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcf2a0a-8476-43c2-a4fa-035afe747311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq: (105555, 24) | Sev: (19646, 18) | Ratio: (105555, 24)\n",
      "\n",
      "Cast a float/int aplicat quan calia.\n",
      "Transformacions log1p aplicades: ['Cost_claims_year', 'Value_vehicle', 'Power']\n",
      "Capping aplicat a: ['Value_vehicle', 'Power', 'Premium', 'Cost_claims_year']\n",
      "Escalat (z-score) aplicat quan calia.\n",
      "Binning aplicat a: ['Driver_age', 'Vehicle_age', 'Power', 'Value_vehicle']\n",
      "Imputació simple final completada (valors NA → -1).\n",
      "Datasets transformats desats a data/processed/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.2 TRANSFORMACIONS DE VARIABLES NUMÈRIQUES\n",
    "# ============================================================\n",
    "# En aquest script faig la part d’enginyeria numèrica avançada pensada\n",
    "# específicament per a models actuarials i de Machine Learning.\n",
    "# Treballo sempre a partir dels dataframes base ja validats, sense\n",
    "# tocar el dataset original del pipeline.\n",
    "#\n",
    "# El que faig en aquest pas és:\n",
    "#  1) Assegurar-me que les variables clau són realment numèriques\n",
    "#  2) Crear transformacions logarítmiques per variables amb cues llargues\n",
    "#  3) Aplicar capping conservador (winsorització) als extrems\n",
    "#  4) Estandarditzar variables per a models basats en gradient\n",
    "#  5) Discretitzar variables per facilitar models lineals/GLM\n",
    "#  6) Fer una imputació final molt simple per evitar NaN residuals\n",
    "#  7) Guardar els datasets transformats llestos per modelització\n",
    "#\n",
    "# Nota important:\n",
    "#  - No faig cap reemplaçament de coma/punt decimal perquè aquí ja\n",
    "#    estic treballant amb el dataset canònic amb decimals en punt.\n",
    "# ============================================================\n",
    "\n",
    "# Importo les llibreries necessàries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler    # Per estandarditzar variables contínues\n",
    "from sklearn.preprocessing import KBinsDiscretizer  # Per fer binning en quantils\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega dels datasets base\n",
    "# ------------------------------------------------------------\n",
    "# Carrego els tres datasets base creats al pas anterior,\n",
    "# cadascun pensat per a un tipus de model diferent.\n",
    "df_freq  = pd.read_csv(\"data/processed/df_freq_base.csv\")    # Base per al model de freqüència\n",
    "df_sev   = pd.read_csv(\"data/processed/df_sev_base.csv\")     # Base per al model de severitat\n",
    "df_ratio = pd.read_csv(\"data/processed/df_ratio_base.csv\")   # Base per al model de ràtio econòmica\n",
    "\n",
    "# Verifico ràpidament dimensions per assegurar-me que tot ha carregat bé\n",
    "print(\"Freq:\", df_freq.shape, \"| Sev:\", df_sev.shape, \"| Ratio:\", df_ratio.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Assegurar que totes les variables clau són numèriques\n",
    "# ------------------------------------------------------------\n",
    "# Definisc una funció auxiliar per forçar conversió a numèric.\n",
    "# Qualsevol valor que no es pugui convertir passa a NaN, i deixo\n",
    "# constància si la conversió genera nuls nous.\n",
    "def ensure_numeric(df, cols, name=\"df\"):\n",
    "    \"\"\"\n",
    "    Força la conversió de les columnes indicades a tipus numèric.\n",
    "    Si algun valor no és convertible, es transforma a NaN.\n",
    "    També informo si la conversió crea NaN nous.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            before_nulls = df[col].isna().sum()\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            after_nulls = df[col].isna().sum()\n",
    "            new_nans = after_nulls - before_nulls\n",
    "            if new_nans > 0:\n",
    "                print(f\"{name} — La columna '{col}' ha generat {new_nans} NaN nous en convertir a numèrica.\")\n",
    "    return df\n",
    "\n",
    "# Llista de variables que vull garantir com a numèriques\n",
    "numeric_cols = [\n",
    "    \"Driver_age\",                # Edat del conductor\n",
    "    \"Licence_age\",               # Antiguitat del carnet\n",
    "    \"Vehicle_age\",               # Antiguitat del vehicle\n",
    "    \"Power\",                     # Potència del vehicle\n",
    "    \"Value_vehicle\",             # Valor assegurat del vehicle\n",
    "    \"Premium\",                   # Prima anual\n",
    "    \"Cylinder_capacity\",         # Cilindrada\n",
    "    \"Weight\",                    # Pes del vehicle\n",
    "    \"Length\",                    # Longitud del vehicle\n",
    "    \"Cost_claims_year\",          # Cost anual de sinistres\n",
    "    \"Claims_to_premium_ratio\"    # Ràtio cost/prima\n",
    "]\n",
    "\n",
    "# Aplico la conversió als tres datasets\n",
    "df_freq  = ensure_numeric(df_freq,  numeric_cols, name=\"df_freq\")\n",
    "df_sev   = ensure_numeric(df_sev,   numeric_cols, name=\"df_sev\")\n",
    "df_ratio = ensure_numeric(df_ratio, numeric_cols, name=\"df_ratio\")\n",
    "\n",
    "print(\"\\nConversió a tipus numèric aplicada quan calia.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Transformacions logarítmiques (log1p)\n",
    "# ------------------------------------------------------------\n",
    "# Aplico transformacions logarítmiques a variables clarament asimètriques,\n",
    "# utilitzant log(1 + x) per evitar problemes amb zeros.\n",
    "log_vars = [\"Cost_claims_year\", \"Value_vehicle\", \"Power\"]\n",
    "\n",
    "for col in log_vars:\n",
    "    if col in df_freq.columns:\n",
    "        df_freq[col + \"_log\"] = np.log1p(df_freq[col])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col + \"_log\"] = np.log1p(df_sev[col])\n",
    "\n",
    "print(\"Transformacions logarítmiques aplicades a:\", log_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Winsorització / Capping\n",
    "# ------------------------------------------------------------\n",
    "# Definisc una funció de capping conservador basada en percentils.\n",
    "# No elimino registres: només limito els valors extrems.\n",
    "def winsorize_series(s, lower=0.01, upper=0.99):\n",
    "    \"\"\"\n",
    "    Aplica capping basat en percentils 1% i 99%.\n",
    "    Serveix per reduir l’impacte d’outliers molt extrems.\n",
    "    \"\"\"\n",
    "    lo = s.quantile(lower)\n",
    "    hi = s.quantile(upper)\n",
    "    return np.clip(s, lo, hi)\n",
    "\n",
    "# Variables on vull aplicar capping\n",
    "winsor_vars = [\"Value_vehicle\", \"Power\", \"Premium\", \"Cost_claims_year\"]\n",
    "\n",
    "for col in winsor_vars:\n",
    "    if col in df_freq.columns:\n",
    "        df_freq[col + \"_cap\"] = winsorize_series(df_freq[col])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col + \"_cap\"] = winsorize_series(df_sev[col])\n",
    "\n",
    "print(\"Capping aplicat a:\", winsor_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Escalat per a models basats en gradient\n",
    "# ------------------------------------------------------------\n",
    "# Estandarditzo algunes variables capejades per facilitar\n",
    "# la convergència de models com GBM, XGBoost, etc.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scale_vars = [\"Value_vehicle_cap\", \"Power_cap\", \"Premium_cap\"]\n",
    "\n",
    "for col in scale_vars:\n",
    "    if col in df_freq.columns:\n",
    "        df_freq[col + \"_scaled\"] = scaler.fit_transform(df_freq[[col]])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col + \"_scaled\"] = scaler.fit_transform(df_sev[[col]])\n",
    "\n",
    "print(\"Estandardització (z-score) aplicada quan calia.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Binning per a models lineals / GLM\n",
    "# ------------------------------------------------------------\n",
    "# Discretitzo algunes variables contínues en quantils\n",
    "# per reforçar relacions més lineals en models tipus GLM.\n",
    "bin_vars = [\"Driver_age\", \"Vehicle_age\", \"Power\", \"Value_vehicle\"]\n",
    "\n",
    "for col in bin_vars:\n",
    "    if col in df_freq.columns:\n",
    "        kb = KBinsDiscretizer(\n",
    "            n_bins=6,\n",
    "            encode=\"ordinal\",\n",
    "            strategy=\"quantile\",\n",
    "            quantile_method=\"linear\"\n",
    "        )\n",
    "        df_freq[col + \"_bin\"] = kb.fit_transform(df_freq[[col]])\n",
    "\n",
    "    if col in df_sev.columns:\n",
    "        kb = KBinsDiscretizer(\n",
    "            n_bins=6,\n",
    "            encode=\"ordinal\",\n",
    "            strategy=\"quantile\",\n",
    "            quantile_method=\"linear\"\n",
    "        )\n",
    "        df_sev[col + \"_bin\"] = kb.fit_transform(df_sev[[col]])\n",
    "\n",
    "print(\"Binning aplicat a:\", bin_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Imputació final simple (NaN → -1)\n",
    "# ------------------------------------------------------------\n",
    "# Com a últim pas defensiu, substitueixo qualsevol NaN residual\n",
    "# per -1. No és una imputació informativa, només evita errors\n",
    "# en models de Machine Learning.\n",
    "df_freq  = df_freq.fillna(-1)\n",
    "df_sev   = df_sev.fillna(-1)\n",
    "df_ratio = df_ratio.fillna(-1)\n",
    "\n",
    "print(\"Imputació final completada (NaN → -1).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Guardar datasets transformats\n",
    "# ------------------------------------------------------------\n",
    "# Deso els datasets ja transformats i llestos per a la fase\n",
    "# de modelització.\n",
    "df_freq.to_csv(\"data/processed/df_freq_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "df_sev.to_csv(\"data/processed/df_sev_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "df_ratio.to_csv(\"data/processed/df_ratio_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Datasets transformats guardats correctament a data/processed/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a0a37-5ca2-40e8-aaa2-a6a6c92a3c30",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.3 Transformació i codificació de variables categòriques</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd473eda-7394-4106-81dd-7c5c4b04ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data carregada:\n",
      "Freq: (105555, 36) | Sev: (19646, 31) | Ratio: (105555, 24)\n",
      "Variables categòriques detectades (df_freq): ['Type_risk', 'Area', 'Type_fuel', 'Distribution_channel', 'Second_driver', 'Has_lapse', 'Has_claims_history']\n",
      "Codificació ordinal aplicada a Type_risk.\n",
      "One-Hot Encoding aplicat (dataset GLM/GAM).\n",
      "Dimensions df_glm: (105555, 47)\n",
      "Target Encoding aplicat (dataset GBM).\n",
      "Dimensions df_gbm: (105555, 37)\n",
      "Datasets categòrics transformats i desats correctament:\n",
      " - data/processed/df_freq_cat_glm.csv\n",
      " - data/processed/df_freq_cat_gbm.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.3 TRANSFORMACIÓ I CODIFICACIÓ DE VARIABLES CATEGÒRIQUES\n",
    "# ============================================================\n",
    "# En aquest script preparo les variables categòriques perquè les pugui\n",
    "# fer servir en diferents famílies de models, mantenint sempre la regla\n",
    "# més important: no contaminar el test (2018) amb informació del train.\n",
    "#\n",
    "# El que faig aquí és:\n",
    "#   - Preparar categòriques per a:\n",
    "#       (A) GLM/GAM  -> One-Hot Encoding (fit només amb TRAIN)\n",
    "#       (B) GBM      -> Target Encoding (calculat només amb TRAIN)\n",
    "#   - Aplicar codificació ordinal a Type_risk (ordre natural 1<2<3<4)\n",
    "#   - Tenir el dataset exportat en formats separats segons el model\n",
    "#\n",
    "# Notes crítiques que m'asseguro de respectar:\n",
    "#   - Tant OHE com Target Encoding els fitjo/estimo només amb TRAIN.\n",
    "#   - Mai calculo target encoding amb tot el dataset perquè això seria leakage temporal.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Creo la carpeta de sortida si no existeix\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega dels datasets numèrics transformats (3.3.4.2)\n",
    "# ------------------------------------------------------------\n",
    "# Aquí carrego el dataset de freqüència ja amb transformacions numèriques aplicades.\n",
    "# Els datasets de severitat i ràtio els assumisc ja carregats d'abans (seguint el pipeline),\n",
    "# però igualment els valido després.\n",
    "df_freq  = pd.read_csv(\"data/processed/df_freq_num_transformed.csv\")\n",
    "\n",
    "print(\"Data carregada:\")\n",
    "print(\"Freq:\", df_freq.shape, \"| Sev:\", df_sev.shape, \"| Ratio:\", df_ratio.shape)\n",
    "\n",
    "# Validacions mínimes per assegurar-me que puc fer split per train/test\n",
    "for df_, name in [(df_freq, \"df_freq\"), (df_sev, \"df_sev\"), (df_ratio, \"df_ratio\")]:\n",
    "    if \"set_type\" not in df_.columns:\n",
    "        raise ValueError(f\"{name} no conté 'set_type'. Revisa 3.3.4.1/3.3.4.2.\")\n",
    "    if \"Policy_year\" not in df_.columns:\n",
    "        raise ValueError(f\"{name} no conté 'Policy_year'. Revisa 3.3.4.1/3.3.4.2.\")\n",
    "\n",
    "# Target de freqüència (necessari per al target encoding)\n",
    "target_freq = \"Has_claims_year\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Llistes de variables categòriques\n",
    "# ------------------------------------------------------------\n",
    "# Defineixo quines variables vull tractar com a categòriques.\n",
    "# Després em quedo només amb les que realment existeixen al dataframe.\n",
    "cat_vars = [\n",
    "    \"Type_risk\",\n",
    "    \"Area\",\n",
    "    \"Type_fuel\",\n",
    "    \"Distribution_channel\",\n",
    "    \"Second_driver\",\n",
    "    \"Has_lapse\",\n",
    "    \"Has_claims_history\",\n",
    "]\n",
    "\n",
    "cat_vars = [c for c in cat_vars if c in df_freq.columns]\n",
    "print(\"Variables categòriques detectades (df_freq):\", cat_vars)\n",
    "\n",
    "# Per coherència, converteixo les categòriques a string abans d'entrar a encoders.\n",
    "# Això evita barreges de tipus (p.ex. 1 vs \"1\") i fa que l'OHE/ordinal sigui estable.\n",
    "def cast_cats_to_str(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "    return df\n",
    "\n",
    "df_freq  = cast_cats_to_str(df_freq, cat_vars)\n",
    "df_sev   = cast_cats_to_str(df_sev,  [c for c in cat_vars if c in df_sev.columns])\n",
    "df_ratio = cast_cats_to_str(df_ratio,[c for c in cat_vars if c in df_ratio.columns])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Codificació ordinal (Type_risk) — FIT només TRAIN\n",
    "# ------------------------------------------------------------\n",
    "# Type_risk té un ordre natural (1<2<3<4), així que aquí m'interessa\n",
    "# capturar-ho amb una codificació ordinal.\n",
    "# Si apareix alguna etiqueta fora d'aquest conjunt, la codifico com -1.\n",
    "if \"Type_risk\" in df_freq.columns:\n",
    "    ord_enc = OrdinalEncoder(\n",
    "        categories=[[\"1\", \"2\", \"3\", \"4\"]],\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1\n",
    "    )\n",
    "\n",
    "    # Defineixo el train mask per freq (és el que fa de referència per fit)\n",
    "    train_mask_freq = df_freq[\"set_type\"].astype(str) == \"train\"\n",
    "\n",
    "    # Fit només amb TRAIN per evitar leakage temporal\n",
    "    ord_enc.fit(df_freq.loc[train_mask_freq, [\"Type_risk\"]])\n",
    "\n",
    "    # Transformo tot el dataset de freq (train+test) amb l'encoder ja fitjat\n",
    "    df_freq[\"Type_risk_ord\"] = ord_enc.transform(df_freq[[\"Type_risk\"]])\n",
    "\n",
    "    # Reutilitzo el mateix encoder per sev i ratio (mateix mapping, fitjat amb train de freq)\n",
    "    if \"Type_risk\" in df_sev.columns:\n",
    "        df_sev[\"Type_risk_ord\"] = ord_enc.transform(df_sev[[\"Type_risk\"]])\n",
    "    if \"Type_risk\" in df_ratio.columns:\n",
    "        df_ratio[\"Type_risk_ord\"] = ord_enc.transform(df_ratio[[\"Type_risk\"]])\n",
    "\n",
    "print(\"Codificació ordinal aplicada a Type_risk.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) One-Hot Encoding per GLM/GAM — FIT només TRAIN, TRANSFORM a tot\n",
    "# ------------------------------------------------------------\n",
    "# Per a models lineals/GLM/GAM, el que necessito és una matriu de dummies.\n",
    "# Aquí faig fit només amb TRAIN per definir l'espai de categories.\n",
    "glm_cat_vars = cat_vars.copy()\n",
    "\n",
    "# Em preparo un OneHotEncoder compatible amb versions diferents de sklearn\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Treballo sobre una còpia per separar clarament \"dataset GLM\"\n",
    "df_glm = df_freq.copy()\n",
    "\n",
    "train_mask = df_glm[\"set_type\"].astype(str) == \"train\"\n",
    "\n",
    "# Fit només amb TRAIN (categories definides amb dades fins 2017)\n",
    "ohe.fit(df_glm.loc[train_mask, glm_cat_vars])\n",
    "\n",
    "# Transformo tot (train+test) amb el mateix encoder (categories no vistes → tot a 0)\n",
    "glm_encoded = ohe.transform(df_glm[glm_cat_vars])\n",
    "ohe_cols = ohe.get_feature_names_out(glm_cat_vars)\n",
    "\n",
    "# Construeixo DataFrame amb les dummies i el concateno\n",
    "df_ohe = pd.DataFrame(glm_encoded, columns=ohe_cols, index=df_glm.index)\n",
    "df_glm = pd.concat([df_glm.drop(columns=glm_cat_vars), df_ohe], axis=1)\n",
    "\n",
    "print(\"One-Hot Encoding aplicat (dataset GLM/GAM).\")\n",
    "print(\"Dimensions df_glm:\", df_glm.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Target Encoding per GBM — calculat només amb TRAIN, aplicat a tot\n",
    "# ------------------------------------------------------------\n",
    "# Per models tipus GBM, un target encoding pot ser útil per capturar\n",
    "# informació agregada de categories sense explotar dimensionalitat.\n",
    "# Sempre el calculo NOMÉS amb TRAIN.\n",
    "def target_encode_train_only(df, col, target, set_col=\"set_type\"):\n",
    "    \"\"\"\n",
    "    Target encoding simple:\n",
    "      - calcula la mitjana del target per categoria amb TRAIN\n",
    "      - aplica al dataset complet\n",
    "      - categories no vistes -> global mean (TRAIN)\n",
    "    \"\"\"\n",
    "    train_mask = df[set_col].astype(str) == \"train\"\n",
    "    global_mean = df.loc[train_mask, target].mean()\n",
    "\n",
    "    means = df.loc[train_mask].groupby(col)[target].mean()\n",
    "    encoded = df[col].map(means)\n",
    "\n",
    "    return encoded.fillna(global_mean)\n",
    "\n",
    "# Creo el dataset específic per GBM sobre una còpia del df_freq\n",
    "df_gbm = df_freq.copy()\n",
    "\n",
    "# Aplico target encoding a totes les variables categòriques definides\n",
    "# (incloent binàries, tot i que aquí és una decisió més pràctica que teòrica).\n",
    "for col in cat_vars:\n",
    "    df_gbm[col + \"_te\"] = target_encode_train_only(df_gbm, col, target_freq, set_col=\"set_type\")\n",
    "\n",
    "# Un cop tinc les versions _te, puc eliminar les categòriques originals\n",
    "df_gbm = df_gbm.drop(columns=cat_vars)\n",
    "\n",
    "print(\"Target Encoding aplicat (dataset GBM).\")\n",
    "print(\"Dimensions df_gbm:\", df_gbm.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Guardar datasets finals (freq)\n",
    "# ------------------------------------------------------------\n",
    "# Exporto els dos datasets finals per freqüència:\n",
    "#   - df_glm: per GLM/GAM amb dummies\n",
    "#   - df_gbm: per GBM amb target encoding\n",
    "df_glm.to_csv(\"data/processed/df_freq_cat_glm.csv\", index=False, encoding=\"utf-8\")\n",
    "df_gbm.to_csv(\"data/processed/df_freq_cat_gbm.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Datasets categòrics transformats i desats correctament:\")\n",
    "print(\" - data/processed/df_freq_cat_glm.csv\")\n",
    "print(\" - data/processed/df_freq_cat_gbm.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a7d1a-e548-437b-bc87-27dae3c473d1",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.4\tVariables derivades i indicadors compostos</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e15f8e8d-7d93-4a67-baec-7ab43c74af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq GLM: (105555, 47)\n",
      "Freq GBM: (105555, 37)\n",
      "Severitat: (19646, 31)\n",
      "\n",
      "[Freqüència - GLM] Interaccions numèriques principals:\n",
      "   ➜ Creat: Driver_age_x_Power\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Value_to_power\n",
      "   ➜ Creat ràtio: Premium_to_value\n",
      "\n",
      "[Freqüència - GBM] Interaccions numèriques principals:\n",
      "   ➜ Creat: Driver_age_x_Power\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Value_to_power\n",
      "   ➜ Creat ràtio: Premium_to_value\n",
      "\n",
      "[Freqüència - GLM] Interaccions Type_risk × Area:\n",
      "   ➜ Creat: Type_risk_ord_x_Area_0\n",
      "   ➜ Creat: Type_risk_ord_x_Area_1\n",
      "\n",
      "[Freqüència - GLM] Interaccions Second_driver × Power:\n",
      "   ➜ Creat: Second_driver_0_x_Power\n",
      "   ➜ Creat: Second_driver_1_x_Power\n",
      "\n",
      "[Freqüència - GBM] Interaccions categòriques resumides:\n",
      "   ➜ Creat: Type_risk_te_x_Area_te\n",
      "   ➜ Creat: Second_driver_te_x_Power\n",
      "\n",
      "[Severitat] Variables derivades i interaccions:\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat: Power_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Sev_cost_to_premium\n",
      "   ➜ Creat ràtio: Sev_value_to_premium\n",
      "   ➜ Creat: Type_risk_x_Area\n",
      "\n",
      "Datasets enriquits desats:\n",
      "   - data/processed/df_freq_fe_glm.csv\n",
      "   - data/processed/df_freq_fe_gbm.csv\n",
      "   - data/processed/df_sev_fe.csv\n",
      "\n",
      "3.3.4.4 complet - Variables derivades i interaccions creades correctament.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.4 VARIABLES DERIVADES I INDICADORS COMPOSTOS\n",
    "# ============================================================\n",
    "# En aquest bloc faig un feature engineering més \"de negoci\" i d'interaccions,\n",
    "# amb l’objectiu de capturar efectes combinats que a l’EDA ja s’intuïen,\n",
    "# però sense caure en una explosió dimensional.\n",
    "#\n",
    "# El que busco aquí és:\n",
    "#   - Relacions conductor × vehicle (interaccions numèriques clares)\n",
    "#   - Ràtios econòmics derivats (prima/valor, valor/potència, etc.)\n",
    "#   - Algunes interaccions amb categòriques ja codificades (GLM: dummies, GBM: target encoding)\n",
    "#   - Mantenir-ho \"selectiu\": poques interaccions però amb sentit actuarial\n",
    "#   - Exportar datasets ja enriquits per passar a 3.3.5 (partició i modelatge)\n",
    "#\n",
    "# Nota:\n",
    "#   - Parteixo de datasets ja transformats:\n",
    "#       * df_freq_cat_glm.csv            → freq per GLM/GAM (OHE)\n",
    "#       * df_freq_cat_gbm.csv            → freq per GBM (target encoding)\n",
    "#       * df_sev_num_transformed.csv     → severitat numèrica transformada\n",
    "# ============================================================\n",
    "\n",
    "# Opcions de display per inspecció en consola\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega de datasets transformats previs\n",
    "# ------------------------------------------------------------\n",
    "# Carrego els fitxers generats a passos anteriors i verifico dimensions\n",
    "# per assegurar-me que estic treballant amb el que toca.\n",
    "freq_glm_path = \"data/processed/df_freq_cat_glm.csv\"          # Freqüència per GLM/GAM\n",
    "freq_gbm_path = \"data/processed/df_freq_cat_gbm.csv\"          # Freqüència per GBM (target encoding)\n",
    "sev_num_path  = \"data/processed/df_sev_num_transformed.csv\"   # Severitat numèrica\n",
    "\n",
    "df_glm = pd.read_csv(freq_glm_path)\n",
    "df_gbm = pd.read_csv(freq_gbm_path)\n",
    "df_sev = pd.read_csv(sev_num_path)\n",
    "\n",
    "print(\"Freq GLM:\", df_glm.shape)\n",
    "print(\"Freq GBM:\", df_gbm.shape)\n",
    "print(\"Severitat:\", df_sev.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Helpers per crear interaccions i ràtios de forma segura\n",
    "# ------------------------------------------------------------\n",
    "# Em creo dues funcions petites per:\n",
    "#   - fer interaccions (multiplicacions) sense petar si falta alguna columna\n",
    "#   - crear ràtios amb control de divisió per zero (epsilon al denominador)\n",
    "def add_interaction(df, col_a, col_b, new_name=None):\n",
    "    \"\"\"\n",
    "    Afegeixo una columna d’interacció = df[col_a] * df[col_b]\n",
    "    només si existeixen les dues columnes al dataframe.\n",
    "    \"\"\"\n",
    "    if col_a in df.columns and col_b in df.columns:\n",
    "        if new_name is None:\n",
    "            new_name = f\"{col_a}_x_{col_b}\"\n",
    "        df[new_name] = df[col_a] * df[col_b]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "    else:\n",
    "        missing = [c for c in [col_a, col_b] if c not in df.columns]\n",
    "        print(f\"   ⚠ No s'ha creat interacció {col_a}×{col_b} (manca: {missing})\")\n",
    "    return df\n",
    "\n",
    "def safe_ratio(df, num_col, den_col, new_name):\n",
    "    \"\"\"\n",
    "    Creo un ràtio num_col / den_col de manera segura:\n",
    "      - només si existeixen les columnes\n",
    "      - afegeixo un epsilon petit al denominador per evitar divisió per 0\n",
    "    \"\"\"\n",
    "    if num_col in df.columns and den_col in df.columns:\n",
    "        df[new_name] = df[num_col] / (df[den_col] + 1e-6)\n",
    "        print(f\"   ➜ Creat ràtio: {new_name}\")\n",
    "    else:\n",
    "        missing = [c for c in [num_col, den_col] if c not in df.columns]\n",
    "        print(f\"   No s'ha creat ràtio {new_name} (manca: {missing})\")\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Interaccions numèriques de risc combinat (freqüència)\n",
    "# ------------------------------------------------------------\n",
    "# Aquí creo interaccions i ràtios que tenen sentit actuarial:\n",
    "#   - combinacions conductor/vehicle\n",
    "#   - proxies econòmiques simples\n",
    "print(\"\\n[Freqüència - GLM] Interaccions numèriques principals:\")\n",
    "\n",
    "# Edat conductor × potència (proxy: perfil del conductor en vehicles més potents)\n",
    "df_glm = add_interaction(df_glm, \"Driver_age\", \"Power\", \"Driver_age_x_Power\")\n",
    "\n",
    "# Antiguitat vehicle × valor vehicle (proxy: valor residual + envelliment)\n",
    "df_glm = add_interaction(df_glm, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "# Valor / potència (proxy: “valor per unitat de potència”)\n",
    "df_glm = safe_ratio(df_glm, \"Value_vehicle\", \"Power\", \"Value_to_power\")\n",
    "\n",
    "# Prima / valor (proxy: intensitat de prima vs capital cobert)\n",
    "df_glm = safe_ratio(df_glm, \"Premium\", \"Value_vehicle\", \"Premium_to_value\")\n",
    "\n",
    "print(\"\\n[Freqüència - GBM] Interaccions numèriques principals:\")\n",
    "\n",
    "df_gbm = add_interaction(df_gbm, \"Driver_age\", \"Power\", \"Driver_age_x_Power\")\n",
    "df_gbm = add_interaction(df_gbm, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "df_gbm = safe_ratio(df_gbm, \"Value_vehicle\", \"Power\", \"Value_to_power\")\n",
    "df_gbm = safe_ratio(df_gbm, \"Premium\", \"Value_vehicle\", \"Premium_to_value\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Interaccions amb variables categòriques codificades\n",
    "# ------------------------------------------------------------\n",
    "# Aquí faig interaccions \"selectives\" amb categòriques ja codificades.\n",
    "#   - GLM: Type_risk_ord × dummies d’Area\n",
    "#   - GLM: dummies de Second_driver × Power\n",
    "#   - GBM: interaccions entre target-encoded (ja són numèriques)\n",
    "print(\"\\n[Freqüència - GLM] Interaccions Type_risk × Area:\")\n",
    "\n",
    "if \"Type_risk_ord\" in df_glm.columns:\n",
    "    area_cols = [c for c in df_glm.columns if c.startswith(\"Area_\")]\n",
    "    for ac in area_cols:\n",
    "        new_name = f\"Type_risk_ord_x_{ac}\"\n",
    "        df_glm[new_name] = df_glm[\"Type_risk_ord\"] * df_glm[ac]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "else:\n",
    "    print(\"   No hi ha Type_risk_ord a df_glm.\")\n",
    "\n",
    "print(\"\\n[Freqüència - GLM] Interaccions Second_driver × Power:\")\n",
    "\n",
    "# Interacció entre dummies de Second_driver i potència\n",
    "power_col = \"Power\"\n",
    "second_driver_cols = [c for c in df_glm.columns if c.startswith(\"Second_driver_\")]\n",
    "\n",
    "for sd in second_driver_cols:\n",
    "    new_name = f\"{sd}_x_{power_col}\"\n",
    "    if power_col in df_glm.columns:\n",
    "        df_glm[new_name] = df_glm[sd] * df_glm[power_col]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "    else:\n",
    "        print(f\"   ⚠ No s'ha creat {new_name} (manca Power).\")\n",
    "\n",
    "print(\"\\n[Freqüència - GBM] Interaccions categòriques resumides:\")\n",
    "\n",
    "# En GBM, les variables _te ja són numèriques, així que puc fer interaccions directes.\n",
    "# Type_risk_te × Area_te\n",
    "if \"Type_risk_te\" in df_gbm.columns and \"Area_te\" in df_gbm.columns:\n",
    "    df_gbm[\"Type_risk_te_x_Area_te\"] = df_gbm[\"Type_risk_te\"] * df_gbm[\"Area_te\"]\n",
    "    print(\"   ➜ Creat: Type_risk_te_x_Area_te\")\n",
    "\n",
    "# Second_driver_te × Power\n",
    "if \"Second_driver_te\" in df_gbm.columns and \"Power\" in df_gbm.columns:\n",
    "    df_gbm[\"Second_driver_te_x_Power\"] = df_gbm[\"Second_driver_te\"] * df_gbm[\"Power\"]\n",
    "    print(\"   ➜ Creat: Second_driver_te_x_Power\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Variables derivades per severitat\n",
    "# ------------------------------------------------------------\n",
    "# Aquí treballo amb df_sev numèric (sense OHE) i creo variables\n",
    "# útils per capturar efectes d’escala i intensitat econòmica.\n",
    "print(\"\\n[Severitat] Variables derivades i interaccions:\")\n",
    "\n",
    "# Antiguitat vehicle × valor vehicle\n",
    "df_sev = add_interaction(df_sev, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "# Potència × valor vehicle (proxy: segment “vehicle potent i car”)\n",
    "df_sev = add_interaction(df_sev, \"Power\", \"Value_vehicle\", \"Power_x_Value_vehicle\")\n",
    "\n",
    "# Cost / prima (intensitat del cost respecte prima)\n",
    "df_sev = safe_ratio(df_sev, \"Cost_claims_year\", \"Premium\", \"Sev_cost_to_premium\")\n",
    "\n",
    "# Valor / prima (capital cobert vs prima)\n",
    "df_sev = safe_ratio(df_sev, \"Value_vehicle\", \"Premium\", \"Sev_value_to_premium\")\n",
    "\n",
    "# Interacció simple Type_risk × Area (si existeixen i té sentit numèric)\n",
    "# Ho deixo com a diagnòstic/interacció simple perquè és una combinació molt \"de negoci\".\n",
    "if \"Type_risk\" in df_sev.columns and \"Area\" in df_sev.columns:\n",
    "    try:\n",
    "        df_sev[\"Type_risk\"] = pd.to_numeric(df_sev[\"Type_risk\"], errors=\"coerce\")\n",
    "        df_sev[\"Type_risk_x_Area\"] = df_sev[\"Type_risk\"] * df_sev[\"Area\"]\n",
    "        print(\"   ➜ Creat: Type_risk_x_Area\")\n",
    "    except Exception as e:\n",
    "        print(\"No s'ha pogut crear Type_risk_x_Area:\", e)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Desar datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "# Exporto els datasets amb el feature engineering aplicat\n",
    "# perquè quedin llestos per a partició i modelatge (3.3.5).\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "freq_glm_fe_path = \"data/processed/df_freq_fe_glm.csv\"\n",
    "freq_gbm_fe_path = \"data/processed/df_freq_fe_gbm.csv\"\n",
    "sev_fe_path      = \"data/processed/df_sev_fe.csv\"\n",
    "\n",
    "df_glm.to_csv(freq_glm_fe_path, index=False)\n",
    "df_gbm.to_csv(freq_gbm_fe_path, index=False)\n",
    "df_sev.to_csv(sev_fe_path, index=False)\n",
    "\n",
    "print(\"\\nDatasets enriquits desats:\")\n",
    "print(\"   -\", freq_glm_fe_path)\n",
    "print(\"   -\", freq_gbm_fe_path)\n",
    "print(\"   -\", sev_fe_path)\n",
    "\n",
    "print(\"\\n3.3.4.4 complet - Variables derivades i interaccions creades correctament.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b11cb-14fe-49ed-ab40-b1017599543a",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.5 Reducció de dimensionalitat i anàlisi multivariable</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b28e57-e226-4110-8b0a-8959789d0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregats:\n",
      "GLM: (105555, 55)\n",
      "GBM: (105555, 43)\n",
      "SEV: (19646, 36)\n",
      "VIF calculat per GLM i guardat a data/processed/vif_glm.csv.\n",
      "Parelles amb correlació alta identificades i desades a high_corr_pairs_glm.csv.\n",
      "PCA exploratori completat i variàncies guardades a pca_variance_gbm.csv.\n",
      "Importàncies GBM calculades i desades a gbm_importances.csv.\n",
      "Variables candidates a eliminació en GLM (VIF>10 o corr>0.85):\n",
      "['Value_vehicle_log', 'Area_0', 'Has_lapse_1', 'Premium', 'Value_vehicle_bin', 'Type_risk_1', 'Type_risk_4', 'Second_driver_1', 'Power_log', 'Driver_age_bin', 'Has_claims_history_0', 'Type_risk_ord', 'Premium_cap', 'Type_fuel_D', 'Vehicle_age_bin', 'Area_1', 'Type_risk_2', 'Value_vehicle_cap', 'Driver_age_x_Power', 'Type_risk_ord_x_Area_0', 'Premium_cap_scaled', 'Second_driver_1_x_Power', 'Value_vehicle_cap_scaled', 'Driver_age', 'Distribution_channel_0', 'Power_cap_scaled', 'Type_risk_3', 'Type_risk_ord_x_Area_1', 'Type_fuel_P', 'Power_cap', 'Has_claims_history_1', 'Has_lapse_0', 'Value_vehicle', 'Power_bin', 'Distribution_channel_1']\n",
      "\n",
      "Arxius exportats a data/processed/:\n",
      " - vif_glm.csv                 (VIF per variable)\n",
      " - high_corr_pairs_glm.csv    (parelles molt correlacionades)\n",
      " - pca_variance_gbm.csv       (variància explicada per PCA en GBM)\n",
      " - gbm_importances.csv        (importància de variables en GBM)\n",
      " - vars_to_drop_glm.csv       (llista de variables a considerar eliminar)\n",
      "\n",
      "3.3.4.5 complet (reducció de dimensionalitat i anàlisi multivariant).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.5 REDUCCIÓ DE DIMENSIONALITAT I ANÀLISI MULTIVARIANT\n",
    "# ============================================================\n",
    "# En aquest script faig un pas de diagnòstic multivariant per entendre\n",
    "# millor la redundància entre predictors i reduir problemes típics en\n",
    "# models lineals (sobretot GLM amb moltes dummies/interaccions).\n",
    "#\n",
    "# El que vull aconseguir és:\n",
    "#   - Mesurar multicol·linearitat (VIF) i correlacions altes al dataset GLM\n",
    "#   - Detectar variables redundants i candidates a eliminar en el GLM\n",
    "#   - Fer un PCA exploratori al dataset GBM només com a diagnòstic\n",
    "#   - Entrenar un Gradient Boosting bàsic per obtenir importàncies\n",
    "#   - Exportar fitxers de suport i una llista de variables \"drop candidates\"\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Carregar datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "# Aquí carrego els datasets sortints de 3.3.4.4:\n",
    "#   - df_freq_fe_glm: freqüència per GLM (OHE + interaccions)\n",
    "#   - df_freq_fe_gbm: freqüència per GBM (target encoding + interaccions)\n",
    "# El dataset de severitat no el faig servir en càlculs aquí, però el\n",
    "# mantinc present al pipeline (per coherència global).\n",
    "df_glm = pd.read_csv(\"data/processed/df_freq_fe_glm.csv\")\n",
    "df_gbm = pd.read_csv(\"data/processed/df_freq_fe_gbm.csv\")\n",
    "\n",
    "print(\"Carregats:\")\n",
    "print(\"GLM:\", df_glm.shape)\n",
    "print(\"GBM:\", df_gbm.shape)\n",
    "print(\"SEV:\", df_sev.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Seleccionar només variables numèriques\n",
    "# ------------------------------------------------------------\n",
    "# Per calcular VIF, correlacions i PCA necessito treballar només amb numèriques.\n",
    "def get_numeric(df):\n",
    "    \"\"\"Retorno només les columnes numèriques (int/float) del DataFrame.\"\"\"\n",
    "    return df.select_dtypes(include=[np.number])\n",
    "\n",
    "num_glm = get_numeric(df_glm)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.1 Eliminar columnes constants o duplicades (abans del VIF)\n",
    "# ------------------------------------------------------------\n",
    "# Abans del VIF elimino:\n",
    "#   - columnes constants (no aporten informació i poden trencar càlculs)\n",
    "#   - columnes duplicades (exactament iguals, generen singularitat)\n",
    "constant_cols = [c for c in num_glm.columns if num_glm[c].nunique() <= 1]\n",
    "duplicated_cols = num_glm.T[num_glm.T.duplicated()].index.tolist()\n",
    "\n",
    "cols_to_remove = set(constant_cols + duplicated_cols)\n",
    "\n",
    "if len(cols_to_remove) > 0:\n",
    "    print(\"Columnes eliminades abans del VIF (constants/duplicades):\")\n",
    "    print(cols_to_remove)\n",
    "\n",
    "# Creo el dataframe net per a VIF i correlacions\n",
    "num_glm_clean = num_glm.drop(columns=list(cols_to_remove), errors=\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Càlcul de VIF amb control d’errors\n",
    "# ------------------------------------------------------------\n",
    "# El VIF mesura multicol·linearitat. En general, un VIF alt indica que\n",
    "# una variable es pot predir molt bé amb la resta (redundància).\n",
    "def compute_vif(df):\n",
    "    \"\"\"\n",
    "    Calculo el VIF per a totes les variables numèriques.\n",
    "    Excloc la variable objectiu si existeix, perquè no té sentit fer VIF del target.\n",
    "    \"\"\"\n",
    "    vif_data = []\n",
    "\n",
    "    # X = predictors numèrics (sense target)\n",
    "    X = df.drop(columns=[\"Has_claims_year\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # Torno a eliminar columnes constants per seguretat\n",
    "    const_cols = X.columns[X.std() == 0]\n",
    "    if len(const_cols) > 0:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    # Calculo VIF variable a variable, controlant warnings típics\n",
    "    for i in range(X.shape[1]):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "            vif = variance_inflation_factor(X.values, i)\n",
    "        vif_data.append((X.columns[i], vif))\n",
    "\n",
    "    vif_df = pd.DataFrame(vif_data, columns=[\"Variable\", \"VIF\"])\n",
    "    # Si algun VIF surt infinit, el converteixo a NaN per no arrossegar problemes després\n",
    "    vif_df[\"VIF\"] = vif_df[\"VIF\"].replace([np.inf, -np.inf], np.nan)\n",
    "    return vif_df\n",
    "\n",
    "# Aplico VIF al subconjunt numèric net\n",
    "vif_glm = compute_vif(num_glm_clean)\n",
    "\n",
    "# Desa a disc per traçabilitat i revisió posterior\n",
    "vif_glm.to_csv(\"data/processed/vif_glm.csv\", index=False)\n",
    "print(\"VIF calculat per GLM i guardat a data/processed/vif_glm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Correlacions altes > |0.75|\n",
    "# ------------------------------------------------------------\n",
    "# Complemento el VIF amb correlacions absolutes altes entre parelles,\n",
    "# perquè això em dona una lectura més directa de redundància bivariant.\n",
    "corr_glm = num_glm_clean.corr().abs()\n",
    "\n",
    "# Agafo només la meitat superior de la matriu (evito duplicats i diagonal)\n",
    "high_corr_pairs = (\n",
    "    corr_glm.where(np.triu(np.ones(corr_glm.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "high_corr_pairs.columns = [\"Var1\", \"Var2\", \"Correlation\"]\n",
    "\n",
    "# Filtre per correlació alta (primer llindar més permissiu)\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs[\"Correlation\"] > 0.75]\n",
    "\n",
    "# Deso parelles correlacionades per poder inspeccionar-ho amb calma\n",
    "high_corr_pairs.to_csv(\"data/processed/high_corr_pairs_glm.csv\", index=False)\n",
    "print(\"Parelles amb correlació alta identificades i desades a high_corr_pairs_glm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) PCA exploratori sobre GBM\n",
    "# ------------------------------------------------------------\n",
    "# Aquí faig PCA només com a diagnòstic per veure quanta variància\n",
    "# capturen els primers components. No ho faig servir com a feature\n",
    "# directe (si ho decideixo més endavant, ja ho justificaré).\n",
    "num_gbm = get_numeric(df_gbm)\n",
    "\n",
    "# X_gbm = predictors numèrics (sense target)\n",
    "X_gbm = num_gbm.drop(columns=[\"Has_claims_year\"], errors=\"ignore\")\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_gbm)\n",
    "\n",
    "pca_variance = pd.DataFrame({\n",
    "    \"Component\": np.arange(1, 6),\n",
    "    \"Explained_variance\": pca.explained_variance_ratio_\n",
    "})\n",
    "pca_variance.to_csv(\"data/processed/pca_variance_gbm.csv\", index=False)\n",
    "\n",
    "print(\"PCA exploratori completat i variàncies guardades a pca_variance_gbm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Importància de variables amb Gradient Boosting (GBM)\n",
    "# ------------------------------------------------------------\n",
    "# Entreno un GradientBoostingClassifier bàsic només com a diagnòstic\n",
    "# per veure quines variables aporten més informació al target.\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "X = X_gbm\n",
    "y = df_gbm[\"Has_claims_year\"]\n",
    "\n",
    "gb.fit(X, y)\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    \"Variable\": X.columns,\n",
    "    \"Importance\": gb.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "importances.to_csv(\"data/processed/gbm_importances.csv\", index=False)\n",
    "print(\"Importàncies GBM calculades i desades a gbm_importances.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Selecció final (regla simple per GLM)\n",
    "# ------------------------------------------------------------\n",
    "# Aquí creo una llista de variables candidates a eliminar al GLM.\n",
    "# Ho faig amb una regla simple i transparent:\n",
    "#   - VIF > 10  → multicol·linearitat alta\n",
    "#   - corr > 0.85 (parelles molt correlacionades) → agafo Var2 com a candidata\n",
    "vars_drop = list(\n",
    "    vif_glm[vif_glm[\"VIF\"] > 10][\"Variable\"].unique()\n",
    ") + list(\n",
    "    high_corr_pairs[high_corr_pairs[\"Correlation\"] > 0.85][\"Var2\"].unique()\n",
    ")\n",
    "\n",
    "# Elimino duplicats perquè una variable pugui sortir per múltiples criteris\n",
    "vars_drop = list(set(vars_drop))\n",
    "\n",
    "# Deso la llista per tenir-la documentada i reutilitzable al pas de modelització\n",
    "pd.Series(vars_drop).to_csv(\"data/processed/vars_to_drop_glm.csv\", index=False)\n",
    "\n",
    "print(\"Variables candidates a eliminació en GLM (VIF>10 o corr>0.85):\")\n",
    "print(vars_drop)\n",
    "\n",
    "print(\"\\nArxius exportats a data/processed/:\")\n",
    "print(\" - vif_glm.csv                 (VIF per variable)\")\n",
    "print(\" - high_corr_pairs_glm.csv    (parelles molt correlacionades)\")\n",
    "print(\" - pca_variance_gbm.csv       (variància explicada per PCA en GBM)\")\n",
    "print(\" - gbm_importances.csv        (importància de variables en GBM)\")\n",
    "print(\" - vars_to_drop_glm.csv       (llista de variables a considerar eliminar)\")\n",
    "\n",
    "print(\"\\n3.3.4.5 complet (reducció de dimensionalitat i anàlisi multivariant).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f546ab-cb11-49a5-a15d-17c2f9c20f94",
   "metadata": {},
   "source": [
    "#### <b>3.3.4.6 Generació del dataset final per models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf1b577-0d42-441d-80dd-247887e28266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregats datasets enriquits:\n",
      "  Freq GLM: (105555, 55)\n",
      "  Freq GBM: (105555, 43)\n",
      "  Severitat: (19646, 36)\n",
      "  Ràtio econòmica: (105555, 24)\n",
      "\n",
      "[Selecció GLM] Variables candidates a eliminació segons 3.3.4.5:\n",
      "  Total a vars_to_drop_glm.csv : 35\n",
      "  Presentes a df_freq_glm      : 35\n",
      "\n",
      "[Selecció GLM] Resum de reducció de dimensionalitat:\n",
      "  Columnes abans (df_freq_glm): 55\n",
      "  Columnes després            : 39\n",
      "  Nº de variables eliminades  : 16\n",
      "  Llista de variables eliminades en df_freq_glm:\n",
      "   - Value_vehicle_log\n",
      "   - Premium\n",
      "   - Value_vehicle_bin\n",
      "   - Power_log\n",
      "   - Driver_age_bin\n",
      "   - Premium_cap\n",
      "   - Vehicle_age_bin\n",
      "   - Value_vehicle_cap\n",
      "   - Driver_age_x_Power\n",
      "   - Premium_cap_scaled\n",
      "   - Value_vehicle_cap_scaled\n",
      "   - Driver_age\n",
      "   - Power_cap_scaled\n",
      "   - Power_cap\n",
      "   - Value_vehicle\n",
      "   - Power_bin\n",
      "\n",
      "Metadades (ID, Policy_year, set_type) presents a tots els datasets principals.\n",
      "\n",
      "Distribució per set_type (Freq GLM):\n",
      "set_type\n",
      "train    69740\n",
      "test     35815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribució per set_type (Severitat):\n",
      "set_type\n",
      "train    16259\n",
      "test      3387\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Datasets complets desats a:\n",
      " - data/model/freq_glm_full.csv (freqüència GLM/GAM, amb reducció aplicada i sense Claims_to_premium_ratio)\n",
      " - data/model/freq_gbm_full.csv (freqüència GBM)\n",
      " - data/model/sev_full.csv (severitat)\n",
      " - data/model/ratio_full.csv (ràtio econòmica, amb Claims_to_premium_ratio com a target)\n",
      "\n",
      "3.3.4.6 complet - Datasets finals consolidats generats.\n",
      " Les decisions de 3.3.4.5 (VIF/correlacions) s'han aplicat al dataset GLM,\n",
      " però preservant explícitament les variables de segmentació actuarial clau.\n",
      " Claims_to_premium_ratio s'utilitza només en el model de ràtio econòmica,\n",
      " i no entra com a predictor en els models de freqüència ni severitat.\n",
      " La divisió train/test (validació temporal) es realitzarà al punt 3.3.6.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.3.4.6 GENERACIÓ DEL DATASET FINAL PER A MODELS\n",
    "# ============================================================\n",
    "# En aquest pas consolido els datasets finals que aniran directament\n",
    "# a la fase de modelització. L’objectiu és deixar-ho tot preparat,\n",
    "# consistent i traçable:\n",
    "#\n",
    "#   - Carrego els datasets enriquits (freq GLM, freq GBM i severitat)\n",
    "#   - Aplico la reducció de dimensionalitat decidida a 3.3.4.5 sobre el GLM\n",
    "#     (VIF/correlacions), però protegint explícitament variables de segmentació\n",
    "#     actuarial perquè no vull perdre els \"talls\" clau de cartera.\n",
    "#   - Em garanteixo que metadades (ID, Policy_year, set_type) i targets hi són\n",
    "#   - Separo el dataset de ràtio econòmica (si existeix) com a dataset independent\n",
    "#   - M’asseguro que Claims_to_premium_ratio no entra com a predictor en freq/sev\n",
    "#   - Exporto els CSV finals a data/model/\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ajusto opcions de display per inspecció ràpida\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "# Carrego els datasets sortints de l'enginyeria de variables:\n",
    "#   - freq GLM: OHE + interaccions\n",
    "#   - freq GBM: target encoding + interaccions\n",
    "#   - severitat: features derivades per al model de cost\n",
    "freq_glm_path = \"data/processed/df_freq_fe_glm.csv\"\n",
    "freq_gbm_path = \"data/processed/df_freq_fe_gbm.csv\"\n",
    "sev_path      = \"data/processed/df_sev_fe.csv\"\n",
    "\n",
    "df_freq_glm = pd.read_csv(freq_glm_path)\n",
    "df_freq_gbm = pd.read_csv(freq_gbm_path)\n",
    "df_sev      = pd.read_csv(sev_path)\n",
    "\n",
    "print(\"Carregats datasets enriquits:\")\n",
    "print(\"  Freq GLM:\", df_freq_glm.shape)\n",
    "print(\"  Freq GBM:\", df_freq_gbm.shape)\n",
    "print(\"  Severitat:\", df_sev.shape)\n",
    "\n",
    "# Dataset de ràtio econòmica (si existeix en aquesta fase)\n",
    "ratio_path = \"data/processed/df_ratio_num_transformed.csv\"\n",
    "if os.path.exists(ratio_path):\n",
    "    df_ratio = pd.read_csv(ratio_path)\n",
    "    print(\"  Ràtio econòmica:\", df_ratio.shape)\n",
    "else:\n",
    "    df_ratio = None\n",
    "    print(\n",
    "        \"  No s'ha trobat df_ratio_num_transformed.csv. \"\n",
    "        \"Es descarta dataset de ràtio en aquesta fase.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Camps clau i targets\n",
    "# ------------------------------------------------------------\n",
    "# Defineixo noms de camps clau per uniformitzar comprovacions.\n",
    "id_col   = \"ID\"\n",
    "year_col = \"Policy_year\"\n",
    "set_col  = \"set_type\"\n",
    "\n",
    "target_freq  = \"Has_claims_year\"\n",
    "target_sev   = \"Cost_claims_year\"\n",
    "target_ratio = \"Claims_to_premium_ratio\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Aplicar decisions de 3.3.4.5 (reducció GLM)\n",
    "#    protegint variables de segmentació actuarial\n",
    "# ------------------------------------------------------------\n",
    "# Aquí aplico la llista de \"drop candidates\" del GLM, però amb una regla clara:\n",
    "#   - mai elimino metadades ni targets\n",
    "#   - mai elimino dummies/columnes que representen segmentació actuarial\n",
    "#     (risc, àrea, combustible, canal, lapse, històric, segon conductor, etc.)\n",
    "vars_drop_path = \"data/processed/vars_to_drop_glm.csv\"\n",
    "\n",
    "if os.path.exists(vars_drop_path):\n",
    "    # Carrego el fitxer com una sèrie (sense capçalera)\n",
    "    vars_drop_series = pd.read_csv(vars_drop_path, header=None)[0]\n",
    "\n",
    "    # Netejo entrades rares (p.ex. blancs, \"0\", etc.)\n",
    "    vars_drop_raw = [\n",
    "        v for v in vars_drop_series.tolist()\n",
    "        if isinstance(v, str) and v.strip() != \"\" and v != \"0\"\n",
    "    ]\n",
    "\n",
    "    # Em quedo només amb les variables que realment existeixen al dataframe GLM\n",
    "    vars_drop_in_glm = [v for v in vars_drop_raw if v in df_freq_glm.columns]\n",
    "\n",
    "    print(\"\\n[Selecció GLM] Variables candidates a eliminació segons 3.3.4.5:\")\n",
    "    print(f\"  Total a vars_to_drop_glm.csv : {len(vars_drop_raw)}\")\n",
    "    print(f\"  Presentes a df_freq_glm      : {len(vars_drop_in_glm)}\")\n",
    "\n",
    "    # Protegeixo explícitament metadades i targets\n",
    "    protected_cols = {\n",
    "        id_col, year_col, set_col,\n",
    "        target_freq, target_sev\n",
    "    }\n",
    "\n",
    "    # També protegeixo variables de segmentació actuarial:\n",
    "    # aquí assumeixo que, en GLM, aquestes variables apareixen com dummies amb prefixos.\n",
    "    segmentation_prefixes = [\n",
    "        \"Type_risk_\",\n",
    "        \"Area_\",\n",
    "        \"Type_fuel_\",\n",
    "        \"Distribution_channel_\",\n",
    "        \"Second_driver_\",\n",
    "        \"Has_lapse_\",\n",
    "        \"Has_claims_history_\",\n",
    "    ]\n",
    "\n",
    "    # Afegeixo a \"protected_cols\" qualsevol columna que comenci amb aquests prefixos\n",
    "    for col in df_freq_glm.columns:\n",
    "        if any(col.startswith(pref) for pref in segmentation_prefixes):\n",
    "            protected_cols.add(col)\n",
    "\n",
    "    # Finalment, només elimino el que està a la llista i NO és protegit\n",
    "    vars_drop_final = [v for v in vars_drop_in_glm if v not in protected_cols]\n",
    "\n",
    "    before_cols = df_freq_glm.shape[1]\n",
    "    df_freq_glm = df_freq_glm.drop(columns=vars_drop_final, errors=\"ignore\")\n",
    "    after_cols = df_freq_glm.shape[1]\n",
    "\n",
    "    print(\"\\n[Selecció GLM] Resum de reducció de dimensionalitat:\")\n",
    "    print(f\"  Columnes abans (df_freq_glm): {before_cols}\")\n",
    "    print(f\"  Columnes després            : {after_cols}\")\n",
    "    print(f\"  Nº de variables eliminades  : {len(vars_drop_final)}\")\n",
    "\n",
    "    if vars_drop_final:\n",
    "        print(\"  Llista de variables eliminades en df_freq_glm:\")\n",
    "        print(\"   - \" + \"\\n   - \".join(vars_drop_final))\n",
    "    else:\n",
    "        print(\"  No s'ha eliminat cap variable (totes protegides o no existents).\")\n",
    "else:\n",
    "    print(\"\\nAVÍS: No s'ha trobat 'vars_to_drop_glm.csv'.\")\n",
    "    print(\"   No s'aplica cap exclusió de variables al dataset GLM.\")\n",
    "    vars_drop_final = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.bis) Assegurar que la ràtio econòmica NO entra al model GLM\n",
    "# ------------------------------------------------------------\n",
    "# Per coherència metodològica: Claims_to_premium_ratio és target d'un model\n",
    "# de rendibilitat, però no l'he de fer servir com a predictor en freq/sev.\n",
    "if target_ratio in df_freq_glm.columns:\n",
    "    print(\n",
    "        f\"\\n[Freq GLM] Eliminant {target_ratio} del dataset GLM \"\n",
    "        \"(només s'utilitzarà com a target del model de ràtio).\"\n",
    "    )\n",
    "    df_freq_glm = df_freq_glm.drop(columns=[target_ratio])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Comprovacions de metadades i targets\n",
    "# ------------------------------------------------------------\n",
    "# Verifico que tots els datasets principals tenen metadades comunes\n",
    "# (ID, Policy_year i set_type), perquè després el split sigui trivial.\n",
    "for name, df_ in [\n",
    "    (\"Freq GLM\", df_freq_glm),\n",
    "    (\"Freq GBM\", df_freq_gbm),\n",
    "    (\"Severitat\", df_sev),\n",
    "]:\n",
    "    missing = [c for c in [id_col, year_col, set_col] if c not in df_.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Falta/n columna/es {missing} al dataset {name}.\")\n",
    "\n",
    "print(\"\\nMetadades (ID, Policy_year, set_type) presents a tots els datasets principals.\")\n",
    "\n",
    "# Targets: aquí no aturo el pipeline si falten, però deixo avís explícit.\n",
    "for name, df_, t in [\n",
    "    (\"Freq GLM\", df_freq_glm, target_freq),\n",
    "    (\"Freq GBM\", df_freq_gbm, target_freq),\n",
    "    (\"Severitat\", df_sev, target_sev),\n",
    "]:\n",
    "    if t not in df_.columns:\n",
    "        print(f\"Avís: el target {t} no es troba al dataset {name}.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Resum temporal (sense split encara)\n",
    "# ------------------------------------------------------------\n",
    "# Només faig un check de distribució perquè em serveix per detectar\n",
    "# errors ràpids (p.ex. tot ha quedat com a train per algun problema).\n",
    "print(\"\\nDistribució per set_type (Freq GLM):\")\n",
    "print(df_freq_glm[set_col].value_counts())\n",
    "\n",
    "print(\"\\nDistribució per set_type (Severitat):\")\n",
    "print(df_sev[set_col].value_counts())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Desar datasets finals\n",
    "# ------------------------------------------------------------\n",
    "# Exporto els datasets finals consolidats a una carpeta \"data/model\".\n",
    "# Encara no faig split train/test aquí; això ho faré explícitament al 3.3.6.\n",
    "os.makedirs(\"data/model\", exist_ok=True)\n",
    "\n",
    "freq_glm_full_path = \"data/model/freq_glm_full.csv\"\n",
    "freq_gbm_full_path = \"data/model/freq_gbm_full.csv\"\n",
    "sev_full_path      = \"data/model/sev_full.csv\"\n",
    "\n",
    "df_freq_glm.to_csv(freq_glm_full_path, index=False)\n",
    "df_freq_gbm.to_csv(freq_gbm_full_path, index=False)\n",
    "df_sev.to_csv(sev_full_path, index=False)\n",
    "\n",
    "print(\"\\nDatasets complets desats a:\")\n",
    "print(\" -\", freq_glm_full_path, \"(freqüència GLM/GAM, amb reducció aplicada i sense Claims_to_premium_ratio)\")\n",
    "print(\" -\", freq_gbm_full_path, \"(freqüència GBM)\")\n",
    "print(\" -\", sev_full_path,      \"(severitat)\")\n",
    "\n",
    "# Dataset de ràtio econòmica separat (si existeix)\n",
    "if df_ratio is not None:\n",
    "    ratio_full_path = \"data/model/ratio_full.csv\"\n",
    "    df_ratio.to_csv(ratio_full_path, index=False)\n",
    "    print(\" -\", ratio_full_path, \"(ràtio econòmica, amb Claims_to_premium_ratio com a target)\")\n",
    "\n",
    "print(\"\\n3.3.4.6 complet - Datasets finals consolidats generats.\")\n",
    "print(\" Les decisions de 3.3.4.5 (VIF/correlacions) s'han aplicat al dataset GLM,\")\n",
    "print(\" però preservant explícitament les variables de segmentació actuarial clau.\")\n",
    "print(\" Claims_to_premium_ratio s'utilitza només en el model de ràtio econòmica,\")\n",
    "print(\" i no entra com a predictor en els models de freqüència ni severitat.\")\n",
    "print(\" La divisió train/test (validació temporal) es realitzarà al punt 3.3.6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b419ba-6cfa-473e-b84e-13e918c78ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_uoc2025 (uoc)",
   "language": "python",
   "name": "environment_uoc2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
