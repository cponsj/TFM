{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2f06da-e12d-40de-a884-c3b17efad992",
   "metadata": {},
   "source": [
    "## <b>4.3 PREPROCESSAMENT I ANÀLISI DE DADES</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cce20-7050-40cd-94c5-e0fdf77fa94e",
   "metadata": {},
   "source": [
    "### <b>4.3.4 Enginyeria de variables avançada</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2334a-8960-4097-bdb6-bc2710ff8b71",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.1 Objectiu i setup</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38da0f1a-dafd-4757-94fe-0dd7373cfbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregat des de: transformed_motor_insurance.csv\n",
      "Dimensions df original: (105555, 44)\n",
      "\n",
      "Variables base de freqüència: 16\n",
      "Variables base de severitat : 11\n",
      "Variables de control       : 4\n",
      "\n",
      "Anys disponibles a Policy_year: [2015, 2016, 2017, 2018]\n",
      "\n",
      "Distribució train/test:\n",
      "set_type\n",
      "train    69740\n",
      "test     35815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribució per any i set_type:\n",
      "Policy_year  set_type\n",
      "2015         train        4559\n",
      "2016         train       31428\n",
      "2017         train       33753\n",
      "2018         test        35815\n",
      "dtype: int64\n",
      "\n",
      "df_freq_base creat (abans de revisar duplicats). Dimensions: (105555, 26)\n",
      "\n",
      "Sense columnes duplicades a df_freq_base. Nombre de columnes: 26\n",
      "df_sev_base creat (abans de revisar duplicats). Dimensions: (19646, 20)\n",
      "\n",
      "Sense columnes duplicades a df_sev_base. Nombre de columnes: 20\n",
      "df_ratio_base creat (abans de revisar duplicats). Dimensions: (105555, 26)\n",
      "\n",
      "Sense columnes duplicades a df_ratio_base. Nombre de columnes: 26\n",
      "\n",
      "--- Valors nuls a df_freq_base ---\n",
      "Policy_duration    70408\n",
      "dtype: int64\n",
      "\n",
      "--- Valors nuls a df_sev_base ---\n",
      "Policy_duration    11936\n",
      "dtype: int64\n",
      "\n",
      "--- Valors nuls a df_ratio_base ---\n",
      "Policy_duration    70408\n",
      "dtype: int64\n",
      "\n",
      "df_freq_base desat a:  data/processed\\df_freq_base.csv\n",
      "df_sev_base desat a:   data/processed\\df_sev_base.csv\n",
      "df_ratio_base desat a: data/processed\\df_ratio_base.csv\n",
      "\n",
      "4.3.4.1 complet - Setup llest per a transformacions avançades (sense columnes duplicades).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.1 ENGINYERIA DE VARIABLES AVANÇADA\n",
    "# SETUP INICIAL I DEFINICIÓ DE DATASETS PER MODEL\n",
    "# ============================================================\n",
    "# Objectiu:\n",
    "#   - Carregar el dataset final preprocessat (transformed_motor_insurance.csv).\n",
    "#   - Definir llistes de variables per model:\n",
    "#       * Model de freqüència (Has_claims_year).\n",
    "#       * Model de severitat (Cost_claims_year).\n",
    "#       * Model de ràtio econòmica (Claims_to_premium_ratio).\n",
    "#   - Crear la variable Policy_year i la partició temporal train/test.\n",
    "#   - Construir df_freq_base, df_sev_base i df_ratio_base com a bases de treball.\n",
    "#   - Comprovar nuls i desar aquests datasets per als subapartats següents.\n",
    "#\n",
    "#   → Versió corregida:\n",
    "#       * S’eviten columnes duplicades en les llistes de variables.\n",
    "#       * Es comprova explícitament si hi ha columnes duplicades\n",
    "#         als DataFrames resultants i, si n’hi ha, es deixen únicament\n",
    "#         les primeres aparicions.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Opcions de display per veure millor taules amples\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Funció auxiliar: assegurar que NO hi ha columnes duplicades\n",
    "# ------------------------------------------------------------\n",
    "def ensure_no_duplicate_columns(df_in: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprova si un DataFrame té columnes duplicades.\n",
    "    - Si NO n’hi ha: només informa i retorna df tal qual.\n",
    "    - Si n’hi ha:\n",
    "        * imprimeix els noms duplicats,\n",
    "        * es queda només amb la primera aparició de cada columna\n",
    "          (df.loc[:, ~df.columns.duplicated()]),\n",
    "        * informa de quantes columnes s’han eliminat.\n",
    "    \"\"\"\n",
    "    dup_mask = df_in.columns.duplicated()\n",
    "    if dup_mask.any():\n",
    "        dup_cols = df_in.columns[dup_mask]\n",
    "        print(f\"\\nATENCIÓ: S'han detectat columnes duplicades a {name}:\")\n",
    "        print(list(dup_cols))\n",
    "\n",
    "        df_out = df_in.loc[:, ~dup_mask].copy()\n",
    "        n_removed = df_in.shape[1] - df_out.shape[1]\n",
    "        print(f\"   → S'han eliminat {n_removed} columna/es duplicada/es de {name}.\")\n",
    "        print(f\"   → Nombre final de columnes a {name}: {df_out.shape[1]}\")\n",
    "        return df_out\n",
    "    else:\n",
    "        print(f\"\\nSense columnes duplicades a {name}. Nombre de columnes: {df_in.shape[1]}\")\n",
    "        return df_in\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega del dataset final\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "data_path = \"transformed_motor_insurance.csv\"\n",
    "\n",
    "# Llegim el dataset principal de treball per als models.\n",
    "# Aquest fitxer és el \"canònic\" després de totes les transformacions prèvies.\n",
    "df = pd.read_csv(\n",
    "    data_path,\n",
    "    sep=\",\",        # separador de camp estàndard CSV\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Dataset carregat des de:\", data_path)\n",
    "print(\"Dimensions df original:\", df.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Definició de variables clau per model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Noms de variables objectiu i identificador\n",
    "target_freq = \"Has_claims_year\"          # target freqüència (0/1)\n",
    "target_sev  = \"Cost_claims_year\"         # target severitat (cost anual)\n",
    "ratio_col   = \"Claims_to_premium_ratio\"  # ràtio econòmica cost/prima\n",
    "id_col      = \"ID\"                       # identificador únic de pòlissa/registre\n",
    "\n",
    "# Llista base de variables explicatives per al model de FREQÜÈNCIA\n",
    "freq_features_base = [\n",
    "    \"Driver_age\", \"Licence_age\", \"Vehicle_age\",\n",
    "    \"Has_lapse\", \"Policy_duration\",\n",
    "    \"Second_driver\", \"Area\", \"Type_risk\", \"Type_fuel\",\n",
    "    \"Has_claims_history\",\n",
    "    \"Value_vehicle\", \"Power\", \"Premium\",\n",
    "    \"Seniority\", \"Policies_in_force\",\n",
    "    \"Distribution_channel\"\n",
    "]\n",
    "\n",
    "# Llista base de variables explicatives per al model de SEVERITAT\n",
    "sev_features_base = [\n",
    "    \"Vehicle_age\", \"Value_vehicle\", \"Power\",\n",
    "    \"Type_risk\", \"Area\", \"Policy_duration\",\n",
    "    \"Claims_to_premium_ratio\",   # també usada com a predictor de severitat\n",
    "    \"Weight\", \"Cylinder_capacity\", \"Premium\", \"Length\"\n",
    "]\n",
    "\n",
    "# Variables de control i qualitat (no necessàriament entren al model com a predictors)\n",
    "# IMPORTANT: aquí s’evita tornar a incloure Has_lapse, que ja és a freq_features_base,\n",
    "#            per no generar duplicats innecessaris a freq_vars.\n",
    "control_features = [\n",
    "    \"Licence_incoherent_flag\",\n",
    "    \"Policy_incoherent_flag\",\n",
    "    \"Lapse_incoherent_flag\",\n",
    "    \"Length_missing_flag\"\n",
    "]\n",
    "\n",
    "print(\"\\nVariables base de freqüència:\", len(freq_features_base))\n",
    "print(\"Variables base de severitat :\", len(sev_features_base))\n",
    "print(\"Variables de control       :\", len(control_features))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Any de pòlissa i partició temporal train/test\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Data de referència per definir l'any de pòlissa\n",
    "date_col = \"Date_last_renewal\"\n",
    "if date_col not in df.columns:\n",
    "    raise ValueError(f\"No existeix {date_col} al dataset. Revisa el fitxer d'entrada.\")\n",
    "\n",
    "# Convertim Date_last_renewal a datetime per poder extreure l'any\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"Policy_year\"] = df[date_col].dt.year\n",
    "\n",
    "# Eliminem registres sense any vàlid (NaN a Policy_year)\n",
    "df = df.dropna(subset=[\"Policy_year\"]).copy()\n",
    "df[\"Policy_year\"] = df[\"Policy_year\"].astype(int)\n",
    "\n",
    "# Llista d'anys disponibles al dataset\n",
    "years = sorted(df[\"Policy_year\"].unique())\n",
    "print(\"\\nAnys disponibles a Policy_year:\", years)\n",
    "\n",
    "# Segons anàlisi temporal prèvia:\n",
    "#   - train: anys anteriors a 2018\n",
    "#   - test : 2018 i posteriors\n",
    "train_years = [y for y in years if y < 2018]\n",
    "test_years  = [y for y in years if y >= 2018]\n",
    "\n",
    "# Etiqueta de conjunt (set_type) per a cada registre\n",
    "df[\"set_type\"] = np.where(df[\"Policy_year\"].isin(train_years), \"train\", \"test\")\n",
    "\n",
    "print(\"\\nDistribució train/test:\")\n",
    "print(df[\"set_type\"].value_counts())\n",
    "\n",
    "print(\"\\nDistribució per any i set_type:\")\n",
    "print(df.groupby([\"Policy_year\", \"set_type\"]).size())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Datasets base per a cada model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ------------------------------\n",
    "# A) FREQÜÈNCIA: df_freq_base\n",
    "# ------------------------------\n",
    "# Inclou:\n",
    "#   - ID, Policy_year, set_type\n",
    "#   - predictors de freq (freq_features_base)\n",
    "#   - targets i ràtio (target_freq, target_sev, ratio_col)\n",
    "#   - flags de control (control_features)\n",
    "freq_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +\n",
    "    freq_features_base +\n",
    "    [target_freq, target_sev, ratio_col] +\n",
    "    control_features\n",
    ")\n",
    "\n",
    "# ❗ PAS IMPORTANT: eliminar duplicats mantenint l’ordre\n",
    "#    (si accidentalment una variable aparegués en més d’una llista)\n",
    "freq_vars = list(dict.fromkeys(freq_vars))\n",
    "\n",
    "# Ens assegurem que només usem columnes que realment existeixen al df\n",
    "freq_vars = [v for v in freq_vars if v in df.columns]\n",
    "\n",
    "# DataFrame base de freqüència\n",
    "df_freq_base = df[freq_vars].copy()\n",
    "print(\"\\ndf_freq_base creat (abans de revisar duplicats). Dimensions:\", df_freq_base.shape)\n",
    "\n",
    "# Comprovem i corrgim columnes duplicades (en principi no n'hi hauria d’haver)\n",
    "df_freq_base = ensure_no_duplicate_columns(df_freq_base, \"df_freq_base\")\n",
    "\n",
    "# ------------------------------\n",
    "# B) SEVERITAT: df_sev_base\n",
    "# ------------------------------\n",
    "# Només sinistres amb cost > 0:\n",
    "#   - Has_claims_year = 1\n",
    "#   - Cost_claims_year != \"0,0\" (segons format original)\n",
    "#   - Cost_claims_year no nul\n",
    "mask_sev = (\n",
    "    (df[target_freq] == 1) &        # només pòlisses amb almenys un sinistre\n",
    "    (df[target_sev] != \"0,0\") &     # excloure sinistres amb cost zero escrit \"0,0\" (si fos text)\n",
    "    (df[target_sev].notna())        # i sense valors nuls\n",
    ")\n",
    "\n",
    "df_sev = df.loc[mask_sev].copy()\n",
    "\n",
    "sev_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +   # identificador i any\n",
    "    sev_features_base +                     # predictors de severitat\n",
    "    [target_sev, target_freq, ratio_col] +  # target de cost + referència de freq i ràtio\n",
    "    control_features                        # flags de qualitat\n",
    ")\n",
    "\n",
    "# Eliminar duplicats mantenint l'ordre (mateix criteri que abans)\n",
    "sev_vars = list(dict.fromkeys(sev_vars))\n",
    "\n",
    "# Ens quedem només amb les columnes que existeixen al subset df_sev\n",
    "sev_vars = [v for v in sev_vars if v in df_sev.columns]\n",
    "\n",
    "# DataFrame base de severitat\n",
    "df_sev_base = df_sev[sev_vars].copy()\n",
    "print(\"df_sev_base creat (abans de revisar duplicats). Dimensions:\", df_sev_base.shape)\n",
    "\n",
    "# Comprovem i corregim columnes duplicades\n",
    "df_sev_base = ensure_no_duplicate_columns(df_sev_base, \"df_sev_base\")\n",
    "\n",
    "# ------------------------------\n",
    "# C) RÀTIO ECONÒMICA: df_ratio_base\n",
    "# ------------------------------\n",
    "# Model amb Claims_to_premium_ratio com a target principal.\n",
    "ratio_vars = (\n",
    "    [id_col, \"Policy_year\", \"set_type\"] +   # info d’identificador i temps\n",
    "    freq_features_base +                    # predictors similars al model de freqüència\n",
    "    [ratio_col, target_freq, target_sev] +  # target de ràtio + referències\n",
    "    control_features                        # flags de control\n",
    ")\n",
    "\n",
    "# Eliminar duplicats mantenint l'ordre\n",
    "ratio_vars = list(dict.fromkeys(ratio_vars))\n",
    "\n",
    "ratio_vars = [v for v in ratio_vars if v in df.columns]\n",
    "\n",
    "# DataFrame base per al model de ràtio econòmica\n",
    "df_ratio_base = df[ratio_vars].copy()\n",
    "print(\"df_ratio_base creat (abans de revisar duplicats). Dimensions:\", df_ratio_base.shape)\n",
    "\n",
    "# Comprovem i corregim columnes duplicades\n",
    "df_ratio_base = ensure_no_duplicate_columns(df_ratio_base, \"df_ratio_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Comprovacions ràpides de nuls\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def quick_missing_report(df_in, name):\n",
    "    \"\"\"\n",
    "    Mostra un petit resum dels valors nuls per DataFrame:\n",
    "      - Llista variables amb almenys 1 nul.\n",
    "      - Ordenades de més a menys nuls.\n",
    "    \"\"\"\n",
    "    missing = df_in.isna().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    print(f\"\\n--- Valors nuls a {name} ---\")\n",
    "    if missing.empty:\n",
    "        print(\"Sense nuls (excepte possibles nuls estructurals).\")\n",
    "    else:\n",
    "        print(missing)\n",
    "\n",
    "# Comprovem nuls als tres datasets base\n",
    "quick_missing_report(df_freq_base,  \"df_freq_base\")\n",
    "quick_missing_report(df_sev_base,   \"df_sev_base\")\n",
    "quick_missing_report(df_ratio_base, \"df_ratio_base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Desa datasets base\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Creem carpeta per a dades processades si no existeix\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Rutes de sortida\n",
    "freq_path  = os.path.join(\"data/processed\", \"df_freq_base.csv\")\n",
    "sev_path   = os.path.join(\"data/processed\", \"df_sev_base.csv\")\n",
    "ratio_path = os.path.join(\"data/processed\", \"df_ratio_base.csv\")\n",
    "\n",
    "# Exportem a CSV (sense índex)\n",
    "df_freq_base.to_csv(freq_path, index=False)\n",
    "df_sev_base.to_csv(sev_path, index=False)\n",
    "df_ratio_base.to_csv(ratio_path, index=False)\n",
    "\n",
    "print(f\"\\ndf_freq_base desat a:  {freq_path}\")\n",
    "print(f\"df_sev_base desat a:   {sev_path}\")\n",
    "print(f\"df_ratio_base desat a: {ratio_path}\")\n",
    "\n",
    "print(\"\\n4.3.4.1 complet - Setup llest per a transformacions avançades (sense columnes duplicades).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ca2af-440b-46d2-9f8a-540d17b291f0",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.2 Transformacions de variables numèriques</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcf2a0a-8476-43c2-a4fa-035afe747311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq: (105555, 26) | Sev: (19646, 20) | Ratio: (105555, 26)\n",
      "\n",
      "Cast a float/int aplicat quan calia.\n",
      "Transformacions log1p aplicades: ['Cost_claims_year', 'Value_vehicle', 'Power']\n",
      "Capping aplicat a: ['Value_vehicle', 'Power', 'Premium', 'Cost_claims_year']\n",
      "Escalat (z-score) aplicat quan calia.\n",
      "Binning aplicat a: ['Driver_age', 'Vehicle_age', 'Power', 'Value_vehicle']\n",
      "Imputació simple final completada (valors NA → -1).\n",
      "Datasets transformats desats a data/processed/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.2 TRANSFORMACIONS DE VARIABLES NUMÈRIQUES\n",
    "# ============================================================\n",
    "# Aquest script té com a objectiu crear transformacions per a models\n",
    "# actuarials i de Machine Learning en l'àmbit d’assegurança de motor.\n",
    "# Les transformacions es construeixen a partir de 3 dataframes base que\n",
    "# prèviament s'han creat després de tractar nuls i incongruències temporals.\n",
    "# \n",
    "# Points clau del procés:\n",
    "#  1) Assegurar que les variables són numèriques reals (float/int)\n",
    "#  2) Crear transformacions logarítmiques per a variables altament asimètriques\n",
    "#  3) Aplicar \"capping\" conservador amb percentils 1% i 99% (winsorització)\n",
    "#  4) Normalitzar (estandarditzar) variables capejades per a models gradient\n",
    "#  5) Binning per a models lineals/GLM per reforçar la relació lineal\n",
    "#  6) Imputació simple final per garantir l'absència de NaN no estructurals\n",
    "#  7) Guardar datasets transformats resultants\n",
    "# \n",
    "# Nota CRUCIAL:\n",
    "#  - NO fem str.replace(\",\", \".\") perquè estem carregant el dataset principal\n",
    "#    amb decimals en punt `.` (transformed_motor_insurance.csv)\n",
    "#  - Els 3 CSV base es van guardar sense modificacions europees de decimals\n",
    "# ============================================================\n",
    "\n",
    "# Importem les llibreries necessàries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler   # Per estandarditzar variables numèriques\n",
    "from sklearn.preprocessing import KBinsDiscretizer # Per discretitzar variables (binning en quantils)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Carrega datasets base\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Carreguem els 3 dataframes base (subsets de variables segons cada model)\n",
    "df_freq = pd.read_csv(\"data/processed/df_freq_base.csv\")   # Taula base per a model de freqüència\n",
    "df_sev  = pd.read_csv(\"data/processed/df_sev_base.csv\")    # Taula base per a model de severitat (només sinistres)\n",
    "df_ratio = pd.read_csv(\"data/processed/df_ratio_base.csv\") # Taula base per a model de ràtio econòmica (cost/prima)\n",
    "\n",
    "# Imprimim dimensions de cada DF\n",
    "print(\"Freq:\", df_freq.shape, \"| Sev:\", df_sev.shape, \"| Ratio:\", df_ratio.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Assegurar que totes les variables clau són numèriques\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def ensure_numeric(df, cols, name=\"df\"):\n",
    "    \"\"\"\n",
    "    Converteix les columnes de `cols` a numèric (float o int segons pandas ho determini).\n",
    "    Si algun valor no és convertible, el transforma a NaN.\n",
    "    També imprimeix quants NaN nous s’han generat per conversió.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:  # Només processem la columna si existeix al dataframe\n",
    "            before_nulls = df[col].isna().sum()  # Comptem nuls abans de convertir\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # Converteix a numèric tolerant errors\n",
    "            after_nulls  = df[col].isna().sum()  # Comptem nuls després de convertir\n",
    "            new_nans = after_nulls - before_nulls  # Diferència = NaN creats en la coerció\n",
    "            if new_nans > 0:\n",
    "                # Avís si la conversió ha creat nous NaN\n",
    "                print(f\"{name} — La columna '{col}' ha generat {new_nans} NaN nous en convertir a numèrica.\")\n",
    "    return df\n",
    "\n",
    "# Llista de columnes numèriques que s’han de garantir com a numèriques\n",
    "numeric_cols = [\n",
    "    \"Driver_age\",        # Edat del conductor (pot estar en float)\n",
    "    \"Licence_age\",       # Antiguitat del carnet (float)\n",
    "    \"Vehicle_age\",       # Antiguitat del vehicle (int o float)\n",
    "    \"Power\",             # Potència del vehicle\n",
    "    \"Value_vehicle\",     # Valor del vehicle assegurat\n",
    "    \"Premium\",           # Prima anual\n",
    "    \"Cylinder_capacity\", # Cilindrada\n",
    "    \"Weight\",            # Pes del vehicle\n",
    "    \"Length\",            # Longitud del vehicle\n",
    "    \"Cost_claims_year\",  # Cost dels sinistres anuals\n",
    "    \"Claims_to_premium_ratio\" # Ràtio cost/prima\n",
    "]\n",
    "\n",
    "# Apliquem el cast/endure numeric als 3 dataframes\n",
    "df_freq  = ensure_numeric(df_freq,  numeric_cols, name=\"df_freq\")\n",
    "df_sev   = ensure_numeric(df_sev,   numeric_cols, name=\"df_sev\")\n",
    "df_ratio = ensure_numeric(df_ratio, numeric_cols, name=\"df_ratio\")\n",
    "\n",
    "print(\"\\nCast a float/int aplicat quan calia.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Transformacions logarítmiques (log1p)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Variables que típicament tenen cues llargues i es beneficien d'un log per estabilitzar variància\n",
    "log_vars = [\"Cost_claims_year\", \"Value_vehicle\", \"Power\"]\n",
    "\n",
    "for col in log_vars:\n",
    "    if col in df_freq.columns:\n",
    "        # log1p = log(1 + x) per evitar errors amb 0 o valors molt petits\n",
    "        # La nova columna porta el sufix \"_log\"\n",
    "        df_freq[col+\"_log\"] = np.log1p(df_freq[col])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col+\"_log\"] = np.log1p(df_sev[col])\n",
    "\n",
    "print(\"Transformacions log1p aplicades:\", log_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Winsorització / Capping\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def winsorize_series(s, lower=0.01, upper=0.99):\n",
    "    \"\"\"\n",
    "    Aplica 'capping' basat en percentils 1% i 99%.\n",
    "    No elimina registres: només limita valors a les cues extremes.\n",
    "    \"\"\"\n",
    "    lo = s.quantile(lower)  # Percentil inferior (1%)\n",
    "    hi = s.quantile(upper)  # Percentil superior (99%)\n",
    "    return np.clip(s, lo, hi)  # Retorna valors tallats entre [lo, hi]\n",
    "\n",
    "# Columnes que winsoritzarem (versió capejada)\n",
    "winsor_vars = [\"Value_vehicle\", \"Power\", \"Premium\", \"Cost_claims_year\"]\n",
    "\n",
    "for col in winsor_vars:\n",
    "    if col in df_freq.columns:\n",
    "        # Generem la versió capejada \"_cap\"\n",
    "        df_freq[col+\"_cap\"] = winsorize_series(df_freq[col])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col+\"_cap\"] = winsorize_series(df_sev[col])\n",
    "\n",
    "print(\"Capping aplicat a:\", winsor_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Escalat per a models basats en gradient (StandardScaler)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Columnes que estandarditzarem (només si existeixen i després d'haver-les capejat)\n",
    "scale_vars = [\"Value_vehicle_cap\", \"Power_cap\", \"Premium_cap\"]\n",
    "\n",
    "for col in scale_vars:\n",
    "    if col in df_freq.columns:\n",
    "        # Fit + transform (s'espera que el DF_freq sigui prou gran per estabilitzar scale)\n",
    "        df_freq[col+\"_scaled\"] = scaler.fit_transform(df_freq[[col]])\n",
    "    if col in df_sev.columns:\n",
    "        df_sev[col+\"_scaled\"] = scaler.fit_transform(df_sev[[col]])\n",
    "\n",
    "print(\"Escalat (z-score) aplicat quan calia.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Binning per models lineals / GLM (KBinsDiscretizer)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Variables que discretitzarem en 6 trams (quantils) buscant linealitat per GLM\n",
    "bin_vars = [\"Driver_age\", \"Vehicle_age\", \"Power\", \"Value_vehicle\"]\n",
    "\n",
    "for col in bin_vars:\n",
    "    if col in df_freq.columns:\n",
    "        # Discretitzem based en quantils, codificat com ordinal\n",
    "        kb = KBinsDiscretizer(\n",
    "            n_bins=6,\n",
    "            encode='ordinal',\n",
    "            strategy='quantile',\n",
    "            quantile_method='linear'  # mètode estable per evitar warnings futurs\n",
    "        )\n",
    "        df_freq[col + \"_bin\"] = kb.fit_transform(df_freq[[col]])\n",
    "\n",
    "    if col in df_sev.columns:\n",
    "        kb = KBinsDiscretizer(\n",
    "            n_bins=6,\n",
    "            encode='ordinal',\n",
    "            strategy='quantile',\n",
    "            quantile_method='linear'\n",
    "        )\n",
    "        df_sev[col + \"_bin\"] = kb.fit_transform(df_sev[[col]])\n",
    "\n",
    "print(\"Binning aplicat a:\", bin_vars)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Imputació final simple (NA → -1)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Substituïm NaN residuals amb -1 per evitar errors als models ML.\n",
    "# És conservador: no introdueix informació nova, només evita fallades de conversió.\n",
    "df_freq  = df_freq.fillna(-1)\n",
    "df_sev   = df_sev.fillna(-1)\n",
    "df_ratio = df_ratio.fillna(-1)\n",
    "\n",
    "print(\"Imputació simple final completada (valors NA → -1).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Guardem datasets transformats\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_freq.to_csv(\"data/processed/df_freq_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "df_sev.to_csv(\"data/processed/df_sev_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "df_ratio.to_csv(\"data/processed/df_ratio_num_transformed.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Datasets transformats desats a data/processed/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a0a37-5ca2-40e8-aaa2-a6a6c92a3c30",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.3 Transformació i codificació de variables categòriques</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd473eda-7394-4106-81dd-7c5c4b04ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data carregada:\n",
      "Freq: (105555, 40) | Sev: (19646, 33) | Ratio: (105555, 26)\n",
      "Categories rares agrupades (si escau) a Type_fuel.\n",
      "Codificació ordinal aplicada a Type_risk.\n",
      "One-Hot Encoding aplicat (dataset GLM/GAM).\n",
      "Dimensions df_glm: (105555, 51)\n",
      "Target Encoding aplicat (dataset Gradient Boosting).\n",
      "Dimensions df_gbm: (105555, 41)\n",
      "Datasets categòrics transformats i desats correctament.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.3 TRANSFORMACIÓ I CODIFICACIÓ DE VARIABLES CATEGÒRIQUES\n",
    "# ============================================================\n",
    "# Objectiu:\n",
    "#   - Preparar les variables categòriques per als diferents models.\n",
    "#   - Aplicar codificació nominal (One-Hot Encoding) per a GLM i GAM.\n",
    "#   - Aplicar Target Encoding per a models basats en gradient (GBM).\n",
    "#   - Realitzar codificació ordinal per a variables amb ordre inherent\n",
    "#     (p. ex. Type_risk).\n",
    "#   - Agrupar categories rares per garantir estabilitat (p. ex. Type_fuel=\"OTHER\").\n",
    "#   - Incorporar informació temporal (freqüències per any) per captar drift.\n",
    "#   - Generar datasets separats per a GLM/GAM i Gradient Boosting.\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega dels datasets numèrics transformats (4.3.4.2)\n",
    "# ------------------------------------------------------------\n",
    "# Aquests fitxers ja inclouen:\n",
    "#   - variables numèriques netes i transformades\n",
    "#   - splits train/test\n",
    "#   - imputació de NaN amb valors sentinel (-1) quan cal\n",
    "\n",
    "df_freq = pd.read_csv(\"data/processed/df_freq_num_transformed.csv\")\n",
    "\n",
    "print(\"Data carregada:\")\n",
    "print(\"Freq:\", df_freq.shape, \"| Sev:\", df_sev.shape, \"| Ratio:\", df_ratio.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Llistes de variables categòriques\n",
    "# ------------------------------------------------------------\n",
    "# Definim les columnes categòriques que volem codificar.\n",
    "# Elles són compartides, principalment, en el model de freqüència.\n",
    "\n",
    "cat_vars = [\n",
    "    \"Type_risk\",            # Tipus de risc (1, 2, 3, 4)\n",
    "    \"Area\",                 # Zona geogràfica\n",
    "    \"Type_fuel\",            # Tipus de combustible\n",
    "    \"Distribution_channel\", # Canal de distribució\n",
    "    \"Second_driver\",        # Indicador segon conductor (0/1)\n",
    "    \"Has_lapse\",            # Indicador de cancel·lació\n",
    "    \"Has_claims_history\"    # Històric de sinistres (0/1)\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Agrupació de categories rares\n",
    "# ------------------------------------------------------------\n",
    "# Funció per agrupar categories amb pes molt baix (< threshold)\n",
    "# en una categoria comuna \"OTHER\", per millorar estabilitat en models.\n",
    "\n",
    "def rare_category_grouping(df, col, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Agrupa categories rares (freqüència relativa < threshold)\n",
    "    en una categoria comuna 'OTHER'.\n",
    "    \"\"\"\n",
    "    freq = df[col].value_counts(normalize=True)  # distribució relativa per categoria\n",
    "    rare_cats = freq[freq < threshold].index     # categories per sota del llindar\n",
    "\n",
    "    if len(rare_cats) > 0:\n",
    "        df[col] = df[col].replace(rare_cats, \"OTHER\")  # substituïm categories rares per \"OTHER\"\n",
    "    return df\n",
    "\n",
    "# Apliquem l’agrupació a Type_fuel (tant a freq, com a sev i ratio)\n",
    "for col in [\"Type_fuel\"]:\n",
    "    # Freqüència\n",
    "    if col in df_freq.columns:\n",
    "        df_freq = rare_category_grouping(df_freq, col)\n",
    "\n",
    "    # Severitat\n",
    "    if col in df_sev.columns:\n",
    "        df_sev = rare_category_grouping(df_sev, col)\n",
    "\n",
    "    # Ràtio\n",
    "    if col in df_ratio.columns:\n",
    "        df_ratio = rare_category_grouping(df_ratio, col)\n",
    "\n",
    "print(\"Categories rares agrupades (si escau) a Type_fuel.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Codificació ordinal (Type_risk)\n",
    "# ------------------------------------------------------------\n",
    "# Suposem que Type_risk té un ordre natural: 1 < 2 < 3 < 4.\n",
    "# La codificació ordinal preserva aquesta ordre en una variable numèrica.\n",
    "\n",
    "if \"Type_risk\" in df_freq.columns:\n",
    "    # Definim l'ordre explícit de les categories\n",
    "    ord_enc = OrdinalEncoder(categories=[['1', '2', '3', '4']])\n",
    "\n",
    "    # Fit + transform sobre df_freq (dataset principal)\n",
    "    df_freq[\"Type_risk_ord\"] = ord_enc.fit_transform(df_freq[[\"Type_risk\"]])\n",
    "    # Transform coherent sobre la resta de datasets\n",
    "    df_sev[\"Type_risk_ord\"] = ord_enc.transform(df_sev[[\"Type_risk\"]])\n",
    "    df_ratio[\"Type_risk_ord\"] = ord_enc.transform(df_ratio[[\"Type_risk\"]])\n",
    "\n",
    "print(\"Codificació ordinal aplicada a Type_risk.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) One-Hot Encoding per GLM/GAM\n",
    "# ------------------------------------------------------------\n",
    "# Per a models lineals (GLM/GAM) és habitual usar codificació one-hot,\n",
    "# que converteix cada categoria en una dummy (0/1) per evitar ordres artificials.\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")  # ignorem categories desconegudes en predicció\n",
    "\n",
    "# Copiem df_freq com a base per dataset GLM\n",
    "df_glm = df_freq.copy()\n",
    "\n",
    "# Ajustem el OneHotEncoder sobre les variables categòriques\n",
    "glm_encoded = ohe.fit_transform(df_glm[cat_vars]).toarray()\n",
    "\n",
    "# Obtenim els noms de les noves columnes OHE (una per categoria)\n",
    "ohe_cols = ohe.get_feature_names_out(cat_vars)\n",
    "\n",
    "# Afegim aquestes columnes al DataFrame GLM\n",
    "df_glm[ohe_cols] = glm_encoded\n",
    "\n",
    "# Eliminem les columnes categòriques originals (ara codificades en dummies)\n",
    "df_glm = df_glm.drop(columns=cat_vars)\n",
    "\n",
    "print(\"One-Hot Encoding aplicat (dataset GLM/GAM).\")\n",
    "print(\"Dimensions df_glm:\", df_glm.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Target Encoding per models basats en gradient\n",
    "# ------------------------------------------------------------\n",
    "# Per models tipus Gradient Boosting (XGBoost, LightGBM, CatBoost…),\n",
    "# el Target Encoding és una manera compacta de codificar categories:\n",
    "# substitueix cada categoria pel valor mitjà del target dins d’aquesta categoria.\n",
    "#\n",
    "# Aquí ho apliquem sobre el target de freqüència: Has_claims_year.\n",
    "\n",
    "def target_encode(df, col, target):\n",
    "    \"\"\"\n",
    "    Codifica una variable categòrica `col` substituint cada categoria\n",
    "    per la mitjana del `target` (target encoding simple).\n",
    "    \"\"\"\n",
    "    means = df.groupby(col)[target].mean()  # mitjana del target per categoria\n",
    "    return df[col].map(means)               # mapegem cada fila al valor mitjà corresponent\n",
    "\n",
    "# Base per dataset GBM (freqüència)\n",
    "df_gbm = df_freq.copy()\n",
    "\n",
    "# Apliquem target encoding a cada variable categòrica\n",
    "for col in cat_vars:\n",
    "    df_gbm[col + \"_te\"] = target_encode(df_gbm, col, \"Has_claims_year\")\n",
    "\n",
    "# Eliminem les categòriques originals (treballarem amb les versions _te)\n",
    "df_gbm = df_gbm.drop(columns=cat_vars)\n",
    "\n",
    "print(\"Target Encoding aplicat (dataset Gradient Boosting).\")\n",
    "print(\"Dimensions df_gbm:\", df_gbm.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Guardar datasets finals\n",
    "# ------------------------------------------------------------\n",
    "# Guardem:\n",
    "#   - df_glm: dataset per GLM/GAM amb OHE\n",
    "#   - df_gbm: dataset per models de gradient amb Target Encoding\n",
    "#   - df_glm_temp / df_gbm_temp: versions amb agregats temporals per drift\n",
    "\n",
    "df_glm.to_csv(\"data/processed/df_freq_cat_glm.csv\", index=False)\n",
    "df_gbm.to_csv(\"data/processed/df_freq_cat_gbm.csv\", index=False)\n",
    "\n",
    "print(\"Datasets categòrics transformats i desats correctament.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a7d1a-e548-437b-bc87-27dae3c473d1",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.4\tVariables derivades i indicadors compostos</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15f8e8d-7d93-4a67-baec-7ab43c74af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq GLM: (105555, 51)\n",
      "Freq GBM: (105555, 41)\n",
      "Severitat: (19646, 33)\n",
      "\n",
      "[Freqüència - GLM] Interaccions numèriques principals:\n",
      "   ➜ Creat: Driver_age_x_Power\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Value_to_power\n",
      "   ➜ Creat ràtio: Premium_to_value\n",
      "\n",
      "[Freqüència - GBM] Interaccions numèriques principals:\n",
      "   ➜ Creat: Driver_age_x_Power\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Value_to_power\n",
      "   ➜ Creat ràtio: Premium_to_value\n",
      "\n",
      "[Freqüència - GLM] Interaccions Type_risk × Area:\n",
      "   ➜ Creat: Type_risk_ord_x_Area_0\n",
      "   ➜ Creat: Type_risk_ord_x_Area_1\n",
      "\n",
      "[Freqüència - GLM] Interaccions Second_driver × Power:\n",
      "   ➜ Creat: Second_driver_0_x_Power\n",
      "   ➜ Creat: Second_driver_1_x_Power\n",
      "\n",
      "[Freqüència - GBM] Interaccions categòriques resumides:\n",
      "   ➜ Creat: Type_risk_te_x_Area_te\n",
      "   ➜ Creat: Second_driver_te_x_Power\n",
      "\n",
      "[Severitat] Variables derivades i interaccions:\n",
      "   ➜ Creat: Vehicle_age_x_Value_vehicle\n",
      "   ➜ Creat: Power_x_Value_vehicle\n",
      "   ➜ Creat ràtio: Sev_cost_to_premium\n",
      "   ➜ Creat ràtio: Sev_value_to_premium\n",
      "   ➜ Creat: Type_risk_x_Area\n",
      "\n",
      "Datasets enriquits desats:\n",
      "   - data/processed/df_freq_fe_glm.csv\n",
      "   - data/processed/df_freq_fe_gbm.csv\n",
      "   - data/processed/df_sev_fe.csv\n",
      "\n",
      "4.3.4.4 complet - Variables derivades i interaccions creades correctament.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.4 VARIABLES DERIVADES I INDICADORS COMPOSTOS\n",
    "# ============================================================\n",
    "# Objectiu:\n",
    "#   - Construir variables combinades de risc (driver × vehicle).\n",
    "#   - Crear indicadors de comportament: lapse, recency, historial.\n",
    "#   - Generar ràtios econòmics derivats del valor, potència i prima.\n",
    "#   - Afegir interaccions seleccionades a partir de l’EDA.\n",
    "#   - Evitar explosió dimensional: només interaccions d’alt valor.\n",
    "#   - Guardar datasets per a la partició i modelatge de 4.3.5.\n",
    "#\n",
    "# Nota:\n",
    "#   - Es parteix de datasets ja transformats numèricament i categòricament:\n",
    "#       * df_freq_cat_glm.csv  → per GLM/GAM (OHE)\n",
    "#       * df_freq_cat_gbm.csv  → per GBM (target encoding)\n",
    "#       * df_sev_num_transformed.csv → severitat numèrica transformada\n",
    "# ============================================================\n",
    "\n",
    "# Opcions de display per inspecció en consola\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega de datasets transformats previs\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "freq_glm_path = \"data/processed/df_freq_cat_glm.csv\"          # Freqüència per GLM/GAM\n",
    "freq_gbm_path = \"data/processed/df_freq_cat_gbm.csv\"          # Freqüència per GBM (target encoding)\n",
    "sev_num_path  = \"data/processed/df_sev_num_transformed.csv\"   # Severitat numèrica\n",
    "\n",
    "df_glm = pd.read_csv(freq_glm_path)\n",
    "df_gbm = pd.read_csv(freq_gbm_path)\n",
    "df_sev = pd.read_csv(sev_num_path)\n",
    "\n",
    "print(\"Freq GLM:\", df_glm.shape)\n",
    "print(\"Freq GBM:\", df_gbm.shape)\n",
    "print(\"Severitat:\", df_sev.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Helpers per crear interaccions i ràtios de forma segura\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def add_interaction(df, col_a, col_b, new_name=None):\n",
    "    \"\"\"\n",
    "    Afegeix una nova columna d’interacció = df[col_a] * df[col_b],\n",
    "    si ambdues columnes existeixen al DataFrame.\n",
    "    \"\"\"\n",
    "    if col_a in df.columns and col_b in df.columns:\n",
    "        # Nom per defecte si no se'n passa cap\n",
    "        if new_name is None:\n",
    "            new_name = f\"{col_a}_x_{col_b}\"\n",
    "        df[new_name] = df[col_a] * df[col_b]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "    else:\n",
    "        # Si manca alguna columna, informem i no fem res\n",
    "        missing = [c for c in [col_a, col_b] if c not in df.columns]\n",
    "        print(f\"   ⚠ No s'ha creat interacció {col_a}×{col_b} (manca: {missing})\")\n",
    "    return df\n",
    "\n",
    "def safe_ratio(df, num_col, den_col, new_name):\n",
    "    \"\"\"\n",
    "    Crea un ràtio num_col / den_col si les columnes existeixen.\n",
    "    Afegim un epsilon petit (1e-6) al denominador per evitar divisió per 0.\n",
    "    \"\"\"\n",
    "    if num_col in df.columns and den_col in df.columns:\n",
    "        df[new_name] = df[num_col] / (df[den_col] + 1e-6)\n",
    "        print(f\"   ➜ Creat ràtio: {new_name}\")\n",
    "    else:\n",
    "        missing = [c for c in [num_col, den_col] if c not in df.columns]\n",
    "        print(f\"   No s'ha creat ràtio {new_name} (manca: {missing})\")\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Interaccions numèriques de risc combinat (freqüència)\n",
    "# ------------------------------------------------------------\n",
    "# Interaccions i ràtios pensades per capturar relacions no lineals\n",
    "# entre característiques del conductor, vehicle i economia.\n",
    "\n",
    "print(\"\\n[Freqüència - GLM] Interaccions numèriques principals:\")\n",
    "\n",
    "# Interacció edat conductor × potència\n",
    "df_glm = add_interaction(df_glm, \"Driver_age\", \"Power\", \"Driver_age_x_Power\")\n",
    "\n",
    "# Interacció antiguitat vehicle × valor vehicle\n",
    "df_glm = add_interaction(df_glm, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "# Ràtio valor/potència (proxy de “valor per unitat de potència”)\n",
    "df_glm = safe_ratio(df_glm, \"Value_vehicle\", \"Power\", \"Value_to_power\")\n",
    "\n",
    "# Ràtio prima/valor (intensitat de prima respecte valor assegurat)\n",
    "df_glm = safe_ratio(df_glm, \"Premium\", \"Value_vehicle\", \"Premium_to_value\")\n",
    "\n",
    "print(\"\\n[Freqüència - GBM] Interaccions numèriques principals:\")\n",
    "\n",
    "df_gbm = add_interaction(df_gbm, \"Driver_age\", \"Power\", \"Driver_age_x_Power\")\n",
    "df_gbm = add_interaction(df_gbm, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "df_gbm = safe_ratio(df_gbm, \"Value_vehicle\", \"Power\", \"Value_to_power\")\n",
    "df_gbm = safe_ratio(df_gbm, \"Premium\", \"Value_vehicle\", \"Premium_to_value\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Interaccions amb variables categòriques codificades\n",
    "#    - GLM: Type_risk_ord × dummies d’Area\n",
    "#    - GLM: Second_driver dummificada × Power\n",
    "#    - GBM: interaccions entre columnes target-encoded\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[Freqüència - GLM] Interaccions Type_risk × Area:\")\n",
    "\n",
    "# Per GLM: usem Type_risk_ord (ordinal) i les dummies d’Area (Area_*)\n",
    "if \"Type_risk_ord\" in df_glm.columns:\n",
    "    # Busquem totes les columnes dummies d’Area (creades per l’OHE)\n",
    "    area_cols = [c for c in df_glm.columns if c.startswith(\"Area_\")]\n",
    "    for ac in area_cols:\n",
    "        new_name = f\"Type_risk_ord_x_{ac}\"\n",
    "        df_glm[new_name] = df_glm[\"Type_risk_ord\"] * df_glm[ac]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "else:\n",
    "    print(\"   No hi ha Type_risk_ord a df_glm.\")\n",
    "\n",
    "print(\"\\n[Freqüència - GLM] Interaccions Second_driver × Power:\")\n",
    "\n",
    "# Interacció entre les dummies de Second_driver i la potència\n",
    "power_col = \"Power\"\n",
    "second_driver_cols = [c for c in df_glm.columns if c.startswith(\"Second_driver_\")]\n",
    "for sd in second_driver_cols:\n",
    "    new_name = f\"{sd}_x_{power_col}\"\n",
    "    if power_col in df_glm.columns:\n",
    "        df_glm[new_name] = df_glm[sd] * df_glm[power_col]\n",
    "        print(f\"   ➜ Creat: {new_name}\")\n",
    "    else:\n",
    "        print(f\"   ⚠ No s'ha creat {new_name} (manca Power).\")\n",
    "\n",
    "print(\"\\n[Freqüència - GBM] Interaccions categòriques resumides:\")\n",
    "\n",
    "# Per GBM: treballem amb versions target-encoded (col_te)\n",
    "# Ex.: Type_risk_te × Area_te\n",
    "if \"Type_risk_te\" in df_gbm.columns and \"Area_te\" in df_gbm.columns:\n",
    "    df_gbm[\"Type_risk_te_x_Area_te\"] = df_gbm[\"Type_risk_te\"] * df_gbm[\"Area_te\"]\n",
    "    print(\"   ➜ Creat: Type_risk_te_x_Area_te\")\n",
    "\n",
    "# Ex.: Second_driver_te × Power\n",
    "if \"Second_driver_te\" in df_gbm.columns and \"Power\" in df_gbm.columns:\n",
    "    df_gbm[\"Second_driver_te_x_Power\"] = df_gbm[\"Second_driver_te\"] * df_gbm[\"Power\"]\n",
    "    print(\"   ➜ Creat: Second_driver_te_x_Power\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Variables derivades per severitat\n",
    "#    (treballem sobre df_sev_num_transformed, sense OHE)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[Severitat] Variables derivades i interaccions:\")\n",
    "\n",
    "# Interacció antiguitat vehicle × valor vehicle\n",
    "df_sev = add_interaction(df_sev, \"Vehicle_age\", \"Value_vehicle\", \"Vehicle_age_x_Value_vehicle\")\n",
    "\n",
    "# Interacció potència × valor vehicle\n",
    "df_sev = add_interaction(df_sev, \"Power\", \"Value_vehicle\", \"Power_x_Value_vehicle\")\n",
    "\n",
    "# Ràtio cost/prima (intensitat del cost respecte la prima)\n",
    "df_sev = safe_ratio(df_sev, \"Cost_claims_year\", \"Premium\", \"Sev_cost_to_premium\")\n",
    "\n",
    "# Ràtio valor/prima (relació entre capital cobert i prima)\n",
    "df_sev = safe_ratio(df_sev, \"Value_vehicle\", \"Premium\", \"Sev_value_to_premium\")\n",
    "\n",
    "# Interacció simple Type_risk × Area (si existeixen)\n",
    "if \"Type_risk\" in df_sev.columns and \"Area\" in df_sev.columns:\n",
    "    try:\n",
    "        # Intentem assegurar que Type_risk sigui numèrica\n",
    "        df_sev[\"Type_risk\"] = pd.to_numeric(df_sev[\"Type_risk\"], errors=\"coerce\")\n",
    "        df_sev[\"Type_risk_x_Area\"] = df_sev[\"Type_risk\"] * df_sev[\"Area\"]\n",
    "        print(\"   ➜ Creat: Type_risk_x_Area\")\n",
    "    except Exception as e:\n",
    "        print(\"No s'ha pogut crear Type_risk_x_Area:\", e)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Desa datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "freq_glm_fe_path = \"data/processed/df_freq_fe_glm.csv\"  # freq GLM amb feature engineering\n",
    "freq_gbm_fe_path = \"data/processed/df_freq_fe_gbm.csv\"  # freq GBM amb feature engineering\n",
    "sev_fe_path      = \"data/processed/df_sev_fe.csv\"       # severitat amb feature engineering\n",
    "\n",
    "df_glm.to_csv(freq_glm_fe_path, index=False)\n",
    "df_gbm.to_csv(freq_gbm_fe_path, index=False)\n",
    "df_sev.to_csv(sev_fe_path, index=False)\n",
    "\n",
    "print(\"\\nDatasets enriquits desats:\")\n",
    "print(\"   -\", freq_glm_fe_path)\n",
    "print(\"   -\", freq_gbm_fe_path)\n",
    "print(\"   -\", sev_fe_path)\n",
    "\n",
    "print(\"\\n4.3.4.4 complet - Variables derivades i interaccions creades correctament.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b11cb-14fe-49ed-ab40-b1017599543a",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.5 Reducció de dimensionalitat i anàlisi multivariable</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b28e57-e226-4110-8b0a-8959789d0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregats:\n",
      "GLM: (105555, 59)\n",
      "GBM: (105555, 47)\n",
      "SEV: (19646, 38)\n",
      "VIF calculat per GLM i guardat a data/processed/vif_glm.csv.\n",
      "Parelles amb correlació alta identificades i desades a high_corr_pairs_glm.csv.\n",
      "PCA exploratori completat i variàncies guardades a pca_variance_gbm.csv.\n",
      "Importàncies GBM calculades i desades a gbm_importances.csv.\n",
      "Variables candidates a eliminació en GLM (VIF>10 o corr>0.85):\n",
      "['Type_risk_ord_x_Area_0', 'Premium', 'Type_risk_ord_x_Area_1', 'Claims_to_premium_ratio', 'Driver_age_bin', 'Second_driver_1', 'Has_lapse_1', 'Power_cap', 'Type_risk_2', 'Power_log', 'Value_vehicle_cap_scaled', 'Has_claims_history_1', 'Has_lapse_0', 'Value_vehicle_bin', 'Type_fuel_D', 'Type_fuel_P', 'Value_vehicle_log', 'Type_risk_1', 'Driver_age_x_Power', 'Power_cap_scaled', 'Premium_cap_scaled', 'Power_bin', 'Has_claims_history_0', 'Driver_age', 'Type_risk_ord', 'Value_vehicle', 'Type_risk_4', 'Distribution_channel_1', 'Area_1', 'Type_fuel_Unknown', 'Second_driver_1_x_Power', 'Area_0', 'Value_vehicle_cap', 'Type_risk_3', 'Cost_claims_year_log', 'Premium_cap', 'Distribution_channel_0', 'Vehicle_age_bin']\n",
      "\n",
      "Arxius exportats a data/processed/:\n",
      " - vif_glm.csv                 (VIF per variable)\n",
      " - high_corr_pairs_glm.csv    (parelles molt correlacionades)\n",
      " - pca_variance_gbm.csv       (variància explicada per PCA en GBM)\n",
      " - gbm_importances.csv        (importància de variables en GBM)\n",
      " - vars_to_drop_glm.csv       (llista de variables a considerar eliminar)\n",
      "\n",
      "4.3.4.5 complet (reducció de dimensionalitat i anàlisi multivariant).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.5 REDUCCIÓ DE DIMENSIONALITAT I ANÀLISI MULTIVARIANT\n",
    "# ============================================================\n",
    "# Objectiu:\n",
    "#   - Avaluar la multicol·linearitat entre predictors (VIF, correlacions).\n",
    "#   - Identificar variables redundants i candidates a eliminació per al GLM.\n",
    "#   - Aplicar un PCA exploratori sobre el dataset GBM (només diagnòstic).\n",
    "#   - Avaluar importàncies de variables amb un Gradient Boosting (freqüència).\n",
    "#   - Generar una llista de variables recomanades a eliminar en GLM.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Carregar datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "# Es carreguen els datasets resultants de l’enginyeria de variables:\n",
    "#   - df_freq_fe_glm: model de freqüència per GLM, amb OHE + interaccions\n",
    "#   - df_freq_fe_gbm: model de freqüència per GBM, amb target encoding + interaccions\n",
    "#   - df_sev_fe: model de severitat, no es fa servir directament aquí però es carrega per coherència\n",
    "\n",
    "df_glm = pd.read_csv(\"data/processed/df_freq_fe_glm.csv\")\n",
    "df_gbm = pd.read_csv(\"data/processed/df_freq_fe_gbm.csv\")\n",
    "\n",
    "print(\"Carregats:\")\n",
    "print(\"GLM:\", df_glm.shape)\n",
    "print(\"GBM:\", df_gbm.shape)\n",
    "print(\"SEV:\", df_sev.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Seleccionar només variables numèriques\n",
    "# ------------------------------------------------------------\n",
    "# Per calcular VIF, correlacions i PCA necessitem només columnes numèriques.\n",
    "\n",
    "def get_numeric(df):\n",
    "    \"\"\"Retorna només les columnes numèriques (int/float) del DataFrame.\"\"\"\n",
    "    return df.select_dtypes(include=[np.number])\n",
    "\n",
    "num_glm = get_numeric(df_glm)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.1 Eliminar columnes constants o duplicades (abans del VIF)\n",
    "# ------------------------------------------------------------\n",
    "# Les columnes constants (std=0) o duplicades poden donar problemes al VIF\n",
    "# (divide by zero, inverses de matrius singulars, etc.).\n",
    "# Per això les eliminem prèviament del subconjunt per a GLM.\n",
    "\n",
    "# Columnes constants: tenen <=1 valor diferent\n",
    "constant_cols = [c for c in num_glm.columns if num_glm[c].nunique() <= 1]\n",
    "\n",
    "# Columnes duplicades: vectors idèntics\n",
    "duplicated_cols = num_glm.T[num_glm.T.duplicated()].index.tolist()\n",
    "\n",
    "# Unim totes les columnes a eliminar\n",
    "cols_to_remove = set(constant_cols + duplicated_cols)\n",
    "\n",
    "if len(cols_to_remove) > 0:\n",
    "    print(\"Columnes eliminades abans del VIF (constants/duplicades):\")\n",
    "    print(cols_to_remove)\n",
    "\n",
    "# DataFrame numèric net per càlcul de VIF i correlacions\n",
    "num_glm_clean = num_glm.drop(columns=list(cols_to_remove), errors=\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Càlcul de VIF amb control d’errors\n",
    "# ------------------------------------------------------------\n",
    "# El VIF (Variance Inflation Factor) mesura multicol·linearitat:\n",
    "#   VIF_j = 1 / (1 - R^2_j)   on R^2_j és el R^2 de regressar la variable j\n",
    "#   contra la resta.\n",
    "# VIF > 10 (aprox.) s’acostuma a considerar alta multicol·linearitat.\n",
    "\n",
    "def compute_vif(df):\n",
    "    \"\"\"\n",
    "    Calcula el VIF per a totes les variables numèriques de df,\n",
    "    excloent explícitament la variable objectiu (Has_claims_year) si existeix.\n",
    "    \"\"\"\n",
    "    vif_data = []\n",
    "\n",
    "    # Eliminem la variable objectiu del conjunt X per evitar VIF sobre ella\n",
    "    X = df.drop(columns=[\"Has_claims_year\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # Eliminem de nou columnes amb desviació estàndard 0 per evitar divisions per zero\n",
    "    const_cols = X.columns[X.std() == 0]\n",
    "    if len(const_cols) > 0:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    # Calculem el VIF per a cada columna restant\n",
    "    for i in range(X.shape[1]):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "            vif = variance_inflation_factor(X.values, i)\n",
    "        vif_data.append((X.columns[i], vif))\n",
    "\n",
    "    # Convertim a DataFrame per facilitar export i inspecció\n",
    "    vif_df = pd.DataFrame(vif_data, columns=[\"Variable\", \"VIF\"])\n",
    "    # Substituïm infinits per NaN per evitar problemes posteriors\n",
    "    vif_df[\"VIF\"] = vif_df[\"VIF\"].replace([np.inf, -np.inf], np.nan)\n",
    "    return vif_df\n",
    "\n",
    "# Càlcul de VIF sobre les variables numèriques netes del GLM\n",
    "vif_glm = compute_vif(num_glm_clean)\n",
    "vif_glm.to_csv(\"data/processed/vif_glm.csv\", index=False)\n",
    "\n",
    "print(\"VIF calculat per GLM i guardat a data/processed/vif_glm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Correlacions altes > |0.75|\n",
    "# ------------------------------------------------------------\n",
    "# Un cop netejat num_glm_clean, calculem la matriu de correlació absoluta.\n",
    "# Identificarem parelles de variables amb |correlació| > 0.75 com a candidates\n",
    "# a ser redundants (una de les dues pot ser eliminada al GLM).\n",
    "\n",
    "corr_glm = num_glm_clean.corr().abs()\n",
    "\n",
    "# Ens quedem només amb la part superior de la matriu (k=1 evita diagonal)\n",
    "high_corr_pairs = (\n",
    "    corr_glm.where(np.triu(np.ones(corr_glm.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "high_corr_pairs.columns = [\"Var1\", \"Var2\", \"Correlation\"]\n",
    "\n",
    "# Filtre per correlació alta\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs[\"Correlation\"] > 0.75]\n",
    "\n",
    "# Guardem per anàlisi detallada posterior\n",
    "high_corr_pairs.to_csv(\"data/processed/high_corr_pairs_glm.csv\", index=False)\n",
    "print(\"Parelles amb correlació alta identificades i desades a high_corr_pairs_glm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) PCA exploratori sobre GBM\n",
    "# ------------------------------------------------------------\n",
    "# El PCA es fa només amb finalitats exploratòries: per veure quanta variància\n",
    "# s’explica amb els primers components. No es farà servir necessàriament al model.\n",
    "\n",
    "num_gbm = get_numeric(df_gbm)\n",
    "\n",
    "# Eliminem la variable objectiu de la matriu X\n",
    "X_gbm = num_gbm.drop(columns=[\"Has_claims_year\"], errors=\"ignore\")\n",
    "\n",
    "# Inicialitzem PCA per, per exemple, 5 components\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_gbm)\n",
    "\n",
    "# Guardem la proporció de variància explicada per cada component\n",
    "pca_variance = pd.DataFrame({\n",
    "    \"Component\": np.arange(1, 6),\n",
    "    \"Explained_variance\": pca.explained_variance_ratio_\n",
    "})\n",
    "pca_variance.to_csv(\"data/processed/pca_variance_gbm.csv\", index=False)\n",
    "\n",
    "print(\"PCA exploratori completat i variàncies guardades a pca_variance_gbm.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Importància de variables amb Gradient Boosting (GBM)\n",
    "# ------------------------------------------------------------\n",
    "# Fem servir un GradientBoostingClassifier bàsic per obtenir\n",
    "# importàncies de variables sobre el dataset GBM (freqüència).\n",
    "# Això proporciona un criteri addicional de selecció de variables.\n",
    "\n",
    "gb = GradientBoostingClassifier()  # Model per defecte, només diagnòstic\n",
    "\n",
    "X = X_gbm                          # predictors numèrics (sense Has_claims_year)\n",
    "y = df_gbm[\"Has_claims_year\"]      # target de freqüència\n",
    "\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Extraiem importàncies i les ordenem de més a menys\n",
    "importances = pd.DataFrame({\n",
    "    \"Variable\": X.columns,\n",
    "    \"Importance\": gb.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "importances.to_csv(\"data/processed/gbm_importances.csv\", index=False)\n",
    "print(\"Importàncies GBM calculades i desades a gbm_importances.csv.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Selecció final (regla simple per GLM)\n",
    "# ------------------------------------------------------------\n",
    "# Combinar informació de:\n",
    "#   - VIF > 10   → multicol·linearitat alta\n",
    "#   - Correlació > 0.85 → parelles molt correlacionades\n",
    "#\n",
    "# Creem una llista de variables candidates a eliminar en el model GLM.\n",
    "\n",
    "vars_drop = list(\n",
    "    vif_glm[vif_glm[\"VIF\"] > 10][\"Variable\"].unique()\n",
    ") + list(\n",
    "    high_corr_pairs[high_corr_pairs[\"Correlation\"] > 0.85][\"Var2\"].unique()\n",
    ")\n",
    "\n",
    "# Eliminar duplicats\n",
    "vars_drop = list(set(vars_drop))\n",
    "\n",
    "# Guardar llista de variables candidates a exclusió\n",
    "pd.Series(vars_drop).to_csv(\"data/processed/vars_to_drop_glm.csv\", index=False)\n",
    "\n",
    "print(\"Variables candidates a eliminació en GLM (VIF>10 o corr>0.85):\")\n",
    "print(vars_drop)\n",
    "\n",
    "print(\"\\nArxius exportats a data/processed/:\")\n",
    "print(\" - vif_glm.csv                 (VIF per variable)\")\n",
    "print(\" - high_corr_pairs_glm.csv    (parelles molt correlacionades)\")\n",
    "print(\" - pca_variance_gbm.csv       (variància explicada per PCA en GBM)\")\n",
    "print(\" - gbm_importances.csv        (importància de variables en GBM)\")\n",
    "print(\" - vars_to_drop_glm.csv       (llista de variables a considerar eliminar)\")\n",
    "print(\"\\n4.3.4.5 complet (reducció de dimensionalitat i anàlisi multivariant).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f546ab-cb11-49a5-a15d-17c2f9c20f94",
   "metadata": {},
   "source": [
    "#### <b>4.3.4.6 Generació del dataset final per models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf1b577-0d42-441d-80dd-247887e28266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregats datasets enriquits:\n",
      "  Freq GLM: (105555, 59)\n",
      "  Freq GBM: (105555, 47)\n",
      "  Severitat: (19646, 38)\n",
      "  Ràtio econòmica: (105555, 26)\n",
      "\n",
      "[Selecció GLM] Variables candidates a eliminació segons 4.3.4.5:\n",
      "  Total a vars_to_drop_glm.csv : 38\n",
      "  Presentes a df_freq_glm      : 38\n",
      "\n",
      "[Selecció GLM] Resum de reducció de dimensionalitat:\n",
      "  Columnes abans (df_freq_glm): 59\n",
      "  Columnes després            : 41\n",
      "  Nº de variables eliminades  : 18\n",
      "  Llista de variables eliminades en df_freq_glm:\n",
      "   - Premium\n",
      "   - Claims_to_premium_ratio\n",
      "   - Driver_age_bin\n",
      "   - Power_cap\n",
      "   - Power_log\n",
      "   - Value_vehicle_cap_scaled\n",
      "   - Value_vehicle_bin\n",
      "   - Value_vehicle_log\n",
      "   - Driver_age_x_Power\n",
      "   - Power_cap_scaled\n",
      "   - Premium_cap_scaled\n",
      "   - Power_bin\n",
      "   - Driver_age\n",
      "   - Value_vehicle\n",
      "   - Value_vehicle_cap\n",
      "   - Cost_claims_year_log\n",
      "   - Premium_cap\n",
      "   - Vehicle_age_bin\n",
      "\n",
      "Metadades (ID, Policy_year, set_type) presents a tots els datasets principals.\n",
      "\n",
      "Distribució per set_type (Freq GLM):\n",
      "set_type\n",
      "train    69740\n",
      "test     35815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribució per set_type (Severitat):\n",
      "set_type\n",
      "train    16259\n",
      "test      3387\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Datasets complets desats a:\n",
      " - data/model/freq_glm_full.csv (freqüència GLM/GAM, amb reducció aplicada i sense Claims_to_premium_ratio)\n",
      " - data/model/freq_gbm_full.csv (freqüència GBM)\n",
      " - data/model/sev_full.csv (severitat)\n",
      " - data/model/ratio_full.csv (ràtio econòmica, amb Claims_to_premium_ratio com a target)\n",
      "\n",
      "4.3.4.6 complet - Datasets finals consolidats generats.\n",
      " Les decisions de 4.3.4.5 (VIF/correlacions) s'han aplicat al dataset GLM,\n",
      " però preservant explícitament les variables de segmentació actuarial clau.\n",
      " Claims_to_premium_ratio s'utilitza només en el model de ràtio econòmica,\n",
      " i no entra com a predictor en els models de freqüència ni severitat.\n",
      " La divisió train/test (validació temporal) es realitzarà al punt 4.3.6.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3.4.6 GENERACIÓ DEL DATASET FINAL PER A MODELS\n",
    "# ============================================================\n",
    "# Objectiu:\n",
    "# - Carregar els datasets enriquits després de l'enginyeria de variables:\n",
    "#   * Model de freqüència GLM/GAM (df_freq_fe_glm.csv)\n",
    "#   * Model de freqüència GBM (df_freq_fe_gbm.csv)\n",
    "#   * Model de severitat (df_sev_fe.csv)\n",
    "# - Fer efectives les decisions de 4.3.4.5 (vars_to_drop_glm.csv)\n",
    "#   sobre el dataset GLM (reducció de dimensionalitat i VIF),\n",
    "#   PERÒ protegint explícitament les variables de segmentació actuarial.\n",
    "# - Definir i verificar els camps clau (ID, Policy_year, set_type, targets).\n",
    "# - Generar i desar datasets finals complets per:\n",
    "#   * Model de freqüència (Has_claims_year) – GLM/GAM i Gradient Boosting\n",
    "#   * Model de severitat (Cost_claims_year)\n",
    "#   * Model de rendibilitat (Claims_to_premium_ratio) en un dataset separat\n",
    "#     (ratio_full.csv), sense que aquesta ràtio entri com a predictor en freq/sev.\n",
    "# - Deixar els datasets preparats per aplicar la divisió train/test\n",
    "#   en un punt posterior (4.3.6).\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Càrrega datasets enriquits\n",
    "# ------------------------------------------------------------\n",
    "freq_glm_path = \"data/processed/df_freq_fe_glm.csv\"\n",
    "freq_gbm_path = \"data/processed/df_freq_fe_gbm.csv\"\n",
    "sev_path      = \"data/processed/df_sev_fe.csv\"\n",
    "\n",
    "df_freq_glm = pd.read_csv(freq_glm_path)\n",
    "df_freq_gbm = pd.read_csv(freq_gbm_path)\n",
    "df_sev      = pd.read_csv(sev_path)\n",
    "\n",
    "print(\"Carregats datasets enriquits:\")\n",
    "print(\"  Freq GLM:\", df_freq_glm.shape)\n",
    "print(\"  Freq GBM:\", df_freq_gbm.shape)\n",
    "print(\"  Severitat:\", df_sev.shape)\n",
    "\n",
    "ratio_path = \"data/processed/df_ratio_num_transformed.csv\"\n",
    "if os.path.exists(ratio_path):\n",
    "    df_ratio = pd.read_csv(ratio_path)\n",
    "    print(\"  Ràtio econòmica:\", df_ratio.shape)\n",
    "else:\n",
    "    df_ratio = None\n",
    "    print(\"  No s'ha trobat df_ratio_num_transformed.csv. \"\n",
    "          \"Es descarta dataset de ràtio en aquesta fase.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Camps clau i targets\n",
    "# ------------------------------------------------------------\n",
    "id_col   = \"ID\"\n",
    "year_col = \"Policy_year\"\n",
    "set_col  = \"set_type\"\n",
    "\n",
    "target_freq  = \"Has_claims_year\"\n",
    "target_sev   = \"Cost_claims_year\"\n",
    "target_ratio = \"Claims_to_premium_ratio\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Aplicar decisions de 4.3.4.5 (reducció GLM)\n",
    "#    protegint variables de segmentació actuarial\n",
    "# ------------------------------------------------------------\n",
    "vars_drop_path = \"data/processed/vars_to_drop_glm.csv\"\n",
    "\n",
    "if os.path.exists(vars_drop_path):\n",
    "    vars_drop_series = pd.read_csv(vars_drop_path, header=None)[0]\n",
    "\n",
    "    vars_drop_raw = [\n",
    "        v for v in vars_drop_series.tolist()\n",
    "        if isinstance(v, str) and v.strip() != \"\" and v != \"0\"\n",
    "    ]\n",
    "\n",
    "    vars_drop_in_glm = [v for v in vars_drop_raw if v in df_freq_glm.columns]\n",
    "\n",
    "    print(\"\\n[Selecció GLM] Variables candidates a eliminació segons 4.3.4.5:\")\n",
    "    print(f\"  Total a vars_to_drop_glm.csv : {len(vars_drop_raw)}\")\n",
    "    print(f\"  Presentes a df_freq_glm      : {len(vars_drop_in_glm)}\")\n",
    "\n",
    "    # --- ⬇⬇⬇ CANVI IMPORTANT AQUI ⬇⬇⬇ ---\n",
    "    # Protegim:\n",
    "    #   - metadades i targets\n",
    "    #   - variables de segmentació actuarial (dummies de risc, àrea, combustible, canal, lapse, històric, segon conductor)\n",
    "    protected_cols = {\n",
    "        id_col, year_col, set_col,\n",
    "        target_freq, target_sev\n",
    "    }\n",
    "\n",
    "    # Prefixos de dummies de segmentació a protegir\n",
    "    segmentation_prefixes = [\n",
    "        \"Type_risk_\",\n",
    "        \"Area_\",\n",
    "        \"Type_fuel_\",\n",
    "        \"Distribution_channel_\",\n",
    "        \"Second_driver_\",\n",
    "        \"Has_lapse_\",\n",
    "        \"Has_claims_history_\",\n",
    "    ]\n",
    "\n",
    "    # Afegim totes les columnes de df_freq_glm que comencen per aquests prefixos\n",
    "    for col in df_freq_glm.columns:\n",
    "        if any(col.startswith(pref) for pref in segmentation_prefixes):\n",
    "            protected_cols.add(col)\n",
    "\n",
    "    # A partir d'aquí, només podem eliminar variables que:\n",
    "    #   - surtin a vars_to_drop_in_glm\n",
    "    #   - i NO siguin protegides\n",
    "    vars_drop_final = [v for v in vars_drop_in_glm if v not in protected_cols]\n",
    "    # --- ⬆⬆⬆ FI CANVI IMPORTANT ⬆⬆⬆ ---\n",
    "\n",
    "    before_cols = df_freq_glm.shape[1]\n",
    "    df_freq_glm = df_freq_glm.drop(columns=vars_drop_final, errors=\"ignore\")\n",
    "    after_cols = df_freq_glm.shape[1]\n",
    "\n",
    "    print(\"\\n[Selecció GLM] Resum de reducció de dimensionalitat:\")\n",
    "    print(f\"  Columnes abans (df_freq_glm): {before_cols}\")\n",
    "    print(f\"  Columnes després            : {after_cols}\")\n",
    "    print(f\"  Nº de variables eliminades  : {len(vars_drop_final)}\")\n",
    "    if vars_drop_final:\n",
    "        print(\"  Llista de variables eliminades en df_freq_glm:\")\n",
    "        print(\"   - \" + \"\\n   - \".join(vars_drop_final))\n",
    "    else:\n",
    "        print(\"  No s'ha eliminat cap variable (totes protegides o no existents).\")\n",
    "else:\n",
    "    print(\"\\nAVÍS: No s'ha trobat 'vars_to_drop_glm.csv'.\")\n",
    "    print(\"   No s'aplica cap exclusió de variables al dataset GLM.\")\n",
    "    vars_drop_final = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.bis) Assegurar que la ràtio econòmica NO entra al model GLM\n",
    "# ------------------------------------------------------------\n",
    "if target_ratio in df_freq_glm.columns:\n",
    "    print(f\"\\n[Freq GLM] Eliminant {target_ratio} del dataset GLM \"\n",
    "          \"(només s'utilitzarà com a target del model de ràtio).\")\n",
    "    df_freq_glm = df_freq_glm.drop(columns=[target_ratio])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Comprovacions de metadades i targets\n",
    "# ------------------------------------------------------------\n",
    "for name, df in [\n",
    "    (\"Freq GLM\", df_freq_glm),\n",
    "    (\"Freq GBM\", df_freq_gbm),\n",
    "    (\"Severitat\", df_sev),\n",
    "]:\n",
    "    missing = [c for c in [id_col, year_col, set_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Falta/n columna/es {missing} al dataset {name}.\")\n",
    "print(\"\\nMetadades (ID, Policy_year, set_type) presents a tots els datasets principals.\")\n",
    "\n",
    "for name, df, t in [\n",
    "    (\"Freq GLM\", df_freq_glm, target_freq),\n",
    "    (\"Freq GBM\", df_freq_gbm, target_freq),\n",
    "    (\"Severitat\", df_sev, target_sev),\n",
    "]:\n",
    "    if t not in df.columns:\n",
    "        print(f\"Avís: el target {t} no es troba al dataset {name}.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Resum temporal (sense split encara)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nDistribució per set_type (Freq GLM):\")\n",
    "print(df_freq_glm[set_col].value_counts())\n",
    "\n",
    "print(\"\\nDistribució per set_type (Severitat):\")\n",
    "print(df_sev[set_col].value_counts())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Desar datasets finals\n",
    "# ------------------------------------------------------------\n",
    "os.makedirs(\"data/model\", exist_ok=True)\n",
    "\n",
    "freq_glm_full_path = \"data/model/freq_glm_full.csv\"\n",
    "freq_gbm_full_path = \"data/model/freq_gbm_full.csv\"\n",
    "sev_full_path      = \"data/model/sev_full.csv\"\n",
    "\n",
    "df_freq_glm.to_csv(freq_glm_full_path, index=False)\n",
    "df_freq_gbm.to_csv(freq_gbm_full_path, index=False)\n",
    "df_sev.to_csv(sev_full_path, index=False)\n",
    "\n",
    "print(\"\\nDatasets complets desats a:\")\n",
    "print(\" -\", freq_glm_full_path, \"(freqüència GLM/GAM, amb reducció aplicada i sense Claims_to_premium_ratio)\")\n",
    "print(\" -\", freq_gbm_full_path, \"(freqüència GBM)\")\n",
    "print(\" -\", sev_full_path,      \"(severitat)\")\n",
    "\n",
    "if df_ratio is not None:\n",
    "    ratio_full_path = \"data/model/ratio_full.csv\"\n",
    "    df_ratio.to_csv(ratio_full_path, index=False)\n",
    "    print(\" -\", ratio_full_path, \"(ràtio econòmica, amb Claims_to_premium_ratio com a target)\")\n",
    "\n",
    "print(\"\\n4.3.4.6 complet - Datasets finals consolidats generats.\")\n",
    "print(\" Les decisions de 4.3.4.5 (VIF/correlacions) s'han aplicat al dataset GLM,\")\n",
    "print(\" però preservant explícitament les variables de segmentació actuarial clau.\")\n",
    "print(\" Claims_to_premium_ratio s'utilitza només en el model de ràtio econòmica,\")\n",
    "print(\" i no entra com a predictor en els models de freqüència ni severitat.\")\n",
    "print(\" La divisió train/test (validació temporal) es realitzarà al punt 4.3.6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b419ba-6cfa-473e-b84e-13e918c78ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_uoc2025 (uoc)",
   "language": "python",
   "name": "environment_uoc2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
